\section*{7.60}

Let \rs be iid gamma$(\alpha, \beta)$ with $\alpha$ known. Find the best unbiased estimator of $1/\beta$.

We'll be attempting to use the attainment theorem here. We have iid exponential rvs so it should work out. In hindsight this seemed like the easiest way to approach this, I don't think it was. In a hunt for a shortcut I found more work. Anyway. 

So as always we need to show that

\[
	\frac{d}{d\theta} LL(\theta \mid \vec{x}) = a(\theta)(w(\vec{x}) - \tau(\theta)), \;\; \text{where}\; \tau(\theta) = \frac{1}{\beta}
\]

\begin{align*}
	f_X(x \mid \alpha, \beta) &= \frac{1}{\Gamma(\alpha) \beta^{\alpha}} x^{\alpha - 1} e^{-x / \beta} \\
	L(\alpha, \beta \mid \vec{x}) &= \prod_{i=1}^n \frac{1}{\Gamma(\alpha) \beta^{\alpha}} x_i^{\alpha - 1} e^{-x_i / \beta} \\
	&= (\Gamma(\alpha)\beta^{\alpha})^{-n} e^{-\sum x_i / \beta} \left( \prod x_i \right)^{\alpha - 1} \\
	LL(\alpha, \beta \mid \vec{x}) &= -n(\ln\Gamma(\alpha) + \alpha\ln(\beta)) - \frac{\sum x_i}{\beta} + (\alpha - 1)\ln\left( \prod x_i \right) \\
	\frac{d}{d\beta} LL(\alpha, \beta \mid \vec{x}) &= \frac{-n\alpha}{\beta} + \frac{\sum x_i}{\beta^2} \\
	&= \frac{1}{\beta^2}\left( \sum x_i - \beta n \alpha \right) \\
	&= a(\theta)(w(\vec{x} - \tau(\theta)))
\end{align*}

So now we have the best unbiased estimator for a $\tau(\theta)$, but not the one we need. We have the best unbiased estimator for $n\alpha \beta$. We need the one for $1/\beta$. 

Here we want to utilize the Lehmann-Scheffe theorem. This basically says that if we take an unbiased function of a complete, minimal sufficient statistic we'll get the UMVUE for its expected value. If we can get a function of that statistic where the expected value is $1/\beta$, we're done. 

We want to use $\sum x_i$ for this. In HW3 we found the sufficient statistic for the gamma when both parameters are unknown. Since $\alpha$ is known, $T(\vec{x}) = \left( \sum x_i \right)$ is sufficient for $\beta$. A quick check of the exponential form:

\[
	L(\beta \mid \alpha, \vec{x}) = (\Gamma(\alpha)\beta^{\alpha})^{-n} e^{-\sum x_i / \beta} \left( \prod x_i \right)^{\alpha - 1} 
\]

Shows that $\sum x_i$ are complete as well. $\alpha$ being known means we don't even need to rearrange this. As for it being complete,

\[
	\frac{L(\beta \mid \alpha, \vec{x})}{L(\beta \mid \alpha, \vec{y})} = 
	\frac{(\Gamma(\alpha)\beta^{\alpha})^{-n} e^{-\sum x_i / \beta} \left( \prod y_i \right)^{\alpha - 1} 
}
	{(\Gamma(\alpha)\beta^{\alpha})^{-n} e^{-\sum y_i / \beta} \left( \prod y_i \right)^{\alpha - 1} 
}
\]

is only constant as a function of $\beta$ when $\sum x_i = \sum y_i$. So $T(\vec{x})$ is also minimal. Oh my gosh. Okay. Cool. 

So let us examine $\sum x_i$ real quick to get a handle on this object.

$\sum x_i \sim \text{Gamma}(n\alpha, \beta)$ because of the MGF.

\[
	M_{\sum x_i}(t) = \left( \frac{1}{1 - \beta t} \right)^{n\alpha}
\]

Also, according to the textbook, $1/X$ has an inverse-gamma distribution. According to the BDA3 textbook, the inverse gamma rv has the following expected value (after adjusting for parameterization):

\[
	E[1/X] = \frac{1}{\beta(\alpha-1)}
\]

It follows then that the random variable $1/\sum x_i \sim \text{Inv Gamma}(n\alpha, \beta)$ with an expected value of $(\beta(n\alpha-1))^{-1}$.

Why are we doing all of this? Why do we care? Remember, we're trying to create an unbiased function of $1/\beta$. We just need to scale this function by $n\alpha -1$ to get this to be an unbiased function of $1/\beta$. So we're basically done!

\begin{align*}
	E\left[ \frac{n\alpha - 1}{\sum x_i} \right] &= (n\alpha - 1) E\left[ \frac{1}{\sum x_i} \right] \\
	&= \frac{n\alpha - 1}{\beta(n\alpha - 1)} \\
	&= \frac{1}{\beta}
\end{align*}.

Therefore, the best unbiased estimator of $\tau(\beta) = \frac{1}{\beta}$ is

\[
	w(\vec{x}) = \frac{n\alpha - 1}{\sum x_i}
\]
