---
title: "Homework 2 - Single parameter models"
format: html
self-contained: true
author: Brady Lamson
---

# Problem 1

You would like to know the probability a coin will yield a ‘head’ when spun in a certain manner.  Let $\theta$ denote the probability of a head for a single spin.  Somebody spins the coin 10 times and observes fewer than 3 heads (you only know that there were fewer than 3 successes in 10 spins).  A Beta(4, 4) distribution matches your prior beliefs about $\theta$.

## (a)

Compute the unnormalized posterior density for $\theta$, $p(\theta \mid y < 3)$.

Let's first write out the density of $\theta$.

$$
\begin{align}
    p(\theta) &= \frac{\Gamma(4 + 4)}{\Gamma(4) \Gamma(4)} \theta^{4-1} (1-\theta)^{4-1} \\
    &\propto \theta^3(1-\theta)^3
\end{align}
$$

Next we'll examine $p(y<3|n=10,\theta)$

$$
\begin{align}
    p(y<3\mid n=10,\theta) &= p(y=0\mid n=10,\theta) + p(y=1\mid n=10,\theta) + p(y=2\mid n=10,\theta) \\
    &= \sum_{i=0}^{2} {10 \choose i} \theta^i (1-\theta)^{10-i}
\end{align}
$$

From this,

$$
\begin{align}
    p(\theta \mid y<3) &\propto p(y<3\mid\theta)p(\theta) \\
    &\propto \left( \sum_{i=0}^{2} {10 \choose i} \theta^i (1-\theta)^{10-i} \right) \theta^3(1-\theta)^3 \\
    &\propto \sum_{i=0}^{2} {10 \choose i} \theta^{i+3} (1-\theta)^{13-i} & \left(\text{Move prior into sum}\right)
\end{align}
$$

## (b)

Plot the result from (a).

```{r, echo=FALSE}
func <- function(theta) {
  # Ensure theta is a vector
  likelihood <- sapply(theta, function(theta) {
      i <- 0:2
      n <- 10
      sum(choose(n, i) * theta^(i) * (1 - theta)^(n - i))
  })
  
  # Prior (unnormalized Beta(4,4))
  prior <- theta^3 * (1 - theta)^3
  
  # Unnormalized posterior
  likelihood * prior
}

# Plotting the unnormalized posterior
curve(func, from = 0, to = 1, ylab = "Density", xlab = expression(theta), main="Posterior Distribution", lwd = 2)

```

# Problem 2
Consider two coins $C_1$ and $C_2$.  $P(\mathrm{heads} \mid C_1 ) =0.6$ and $P(\mathrm{heads} \mid C_2 )=0.4$.  Choose one of the coins at random and imagine spinning it repeatedly until you get a head.  Given that the first two spins from the chosen coin are tails, what is the expectation of the number of additional spins until a head shows up?  Let $S$ be the number of additional spins until you spin a ‘head’.  Let $TT$ denote the first two spins were tails.  Determine $E[S|TT]$.  Hint: Use the double expectation rule and Bayes’ theorem to solve this problem.

Okay this problem has a lot going on in it. There are a lot of pieces needed to solve this one and it can be a little overwhelming to understand exactly why each piece is necessary in the moment. At least for me it was! Let's start with our basic building blocks.

First, our two coins. We choose a coin at random, so we can say that $Pr(C_1) = Pr(C_2) = 0.5$. This piece functions as our prior. We assume each coin was equally likely to be selected at the start. In other words, we aren't sure which coin we chose yet. Once we observe our data it'll help us update this belief based on which coin is more likely to grant us those results.

Our data is two tails, or $y=\{TT\}$. We'll come back to this later.

And next is our goal which is to determine $E[S|TT]$ where $S$ represents the number of additional spins until a heads shows up. Based on this interpretation can we state that $S$ follows a geometric distribution.

So what we have here are two possible geometric distributions, one with $p=0.6$ when $C_1$ is chosen and the other with $p=0.4$ for $C_2$. In other words:

$$
\begin{align}
    S\mid TT, C_1 &\sim Geo(0.6) \\
    S\mid TT, C_2 &\sim Geo(0.4) \\
\end{align}
$$

The final answer here will be the *weighted average* of the expectations of the two geometric random variables. The weight will be how likely it was each coin was chosen given that we got two tails so far. 

$$
E[S \mid TT] = p(C_1 \mid TT) \cdot E[S \mid TT, C_1] + p(C_2 \mid TT) \cdot E[S \mid TT, C_2]
$$

Now that we know our goal we can start getting all the pieces together. 

Let's start with the expected values of $S$. These are simple. Given a geometric random variable $S$, $E[S \mid p] = 1/p$. So,

$$
\begin{align}
    E[S \mid TT, C_1] &= \frac{1}{0.6} \\
    E[S \mid TT, C_2] &= \frac{1}{0.4} \\
\end{align}
$$

What's trickier now is the "weights", $p(C_i \mid TT)$. For these we'll want to use bayes rule.

$$
    p(C_i \mid TT) = \frac{p(TT \mid C_i) p(C_i)}{p(TT)}  
$$

Taking on $p(TT)$ will give us what we need for both coins.

$$
p(TT) = p(TT \mid C_1) p(C_1) + p(TT \mid C_2) p(C_2)
$$

$p(TT \mid C_i)$ can be computed using the binomial pmf. Once we have a coin we're simply flipping it. In both cases we know our data, number of trials and probability of success for each trial.

$$
\begin{align}
    p(TT \mid C_1) &= {2 \choose 2} 0.6^0(1-0.6)^2 = 0.4^2 = 0.16 \\
    p(TT \mid C_2) &= {2 \choose 2} 0.4^0(1-0.4)^2 = 0.6^2 = 0.36 \\
\end{align}
$$

So plugging that in gives us:

$$
\begin{align}
    p(TT) &=p(TT \mid C_1) p(C_1) + p(TT \mid C_2) p(C_2) \\
    &= 0.16 \cdot 0.5 + 0.36 \cdot 0.5 \\
    &= 0.08 + 0.18 \\
    &= 0.26
\end{align}
$$

Now we can use all those pieces together.

$$
\begin{align}
    p(C_1 \mid TT) &= \frac{p(TT \mid C_1) p(C_1)}{p(TT)} \\
    &= \frac{0.08}{0.26} \\
    &\approx 0.308 \\
    p(C_2 \mid TT) &= \frac{p(TT \mid C_2) p(C_1)}{p(TT)} \\
    &= \frac{0.18}{0.26} \\
    &\approx 0.692
\end{align}
$$

Lastly, we return back to our goal.

$$
\begin{align}
    E[S \mid TT] &= p(C_1 \mid TT) \cdot E[S \mid TT, C_1] + p(C_2 \mid TT) \cdot E[S \mid TT, C_2] \\
    &\approx 0.308 \cdot \frac{1}{0.6} + 0.692 \cdot \frac{1}{0.4} \\
    &\approx 2.243
\end{align}
$$

To conclude, the expected number of flips it will take to get our first heads is approximately $2.243$. So a little over $2$ flips.

# Problem 3

## (a)
Determine Jeffreys’ prior for $\lambda$ assuming a Poisson($\lambda$) sampling distribution.  

Jeffreys' prior is defined as $p(\theta) \propto J(\theta)^{1/2}$ where $J(\theta)$ is the Fisher Information for $\theta$.

$$J(\theta) = -E\left( \frac{d^2}{d\theta^2} ln(p(y \mid \theta)) \mid \theta\right)$$

So to accomplish this problem we need to go through a few steps. 

1. Get the log-likelihood function
2. Get the first and second derivatives
3. Get the negative expected value of the second derivative
4. Take the square root.

$$
\begin{align}
    p(y \mid \theta) &= \frac{e^{-\theta}\theta^y}{y!} \\
    ln(p(y \mid \theta)) &= -\theta + yln(\theta)-ln(y!) & (\text{log likelihood}) \\
    \frac{d}{d\theta} ln(p(y \mid \theta)) &= -1 + \frac{y}{\theta} & (\text{first derivative}) \\
    \frac{d^2}{d\theta^2} ln(p(y \mid \theta)) &= -\frac{y}{\theta^2} & (\text{second derivative}) \\
    J(\theta) &= -E\left[ -\frac{y}{\theta^2} \mid \theta \right] & (\text{from definition of } J(\theta))\\
    &= \frac{1}{\theta^2} E[y\mid\theta] & (\theta \text{ is fixed}) \\
    &= \frac{1}{\theta^2}\theta & (y\mid\theta \text{ is poisson}) \\
    &= \frac{1}{\theta} \\
    p(\theta) &\propto J(\theta)^{1/2} & (\text{Jeffreys prior def'n})\\
    &\propto \theta^{-1/2}
\end{align}
$$

## (b)

Is the resulting distribution in (a) similar to any known distribution, possibly an "improper" distribution?  If so, with what parameters? 

It looks similar to the kernel of a gamma, almost. Let's look at that kernel and compare.

$$p(\theta \mid \alpha, \beta) \propto \theta^{\alpha - 1}e^{-x/\beta}$$

If $\alpha=1/2$ that first part would be perfect, but as is we can't ignore that second portion that we don't have. As such I believe our prior is improper. 

# Problem 4  

## (a)

Determine the natural parameter of the binomial distribution. 

To get the natural parameter we first need to get the binomial distribution into the exponential form:

$$
p(y \mid \theta) = f(y)g(\theta)e^{\phi(\theta)u(y)}
$$

where $\phi(\theta)$ is our natural parameter. The book gives us a hint, telling us that it's $logit(\theta) = log\left(\frac{\theta}{1-\theta}\right)$ (page 37). So we at least have a goal in mind. 

$$
\begin{align}
    p(y \mid n,\theta) &= {n \choose y} \theta^y (1-\theta)^{n-y} \\
    &= \exp\left(\ln\left({n \choose y} \theta^y (1-\theta)^{n-y}\right)\right) & (\text{Setup for rearranging}) \\
    &= \exp\left(ln{n \choose y} + y \ln(\theta) + (n-y)\ln(1-\theta)\right) & (\text{Product property of ln}) \\
    &= {n \choose y} \exp(y\ln(\theta) - y\ln(1-\theta))\exp(n\ln(1-\theta)) & (\text{Distribute } (n-y), \text{rearrange}) \\
    &= {n \choose y} \exp(n\ln(1-\theta)) \exp\left(y\ln\left(\frac{\theta}{1-\theta}\right)\right) & (\text{Quotient property of ln, rearrange})
\end{align}
$$

So now we have the form we need where:

$$
\begin{align}
    f(y) &= {n \choose y} \\
    g(\theta) &=  \exp(n\ln(1-\theta)) \\
    \phi(\theta) &= \ln\left(\frac{\theta}{1-\theta}\right) \\
    u(y) &= y
\end{align}
$$

As such, our natural paramter is 

$$
\phi(\theta) = \ln\left(\frac{\theta}{1-\theta}\right)
$$

## (b)

Denote the natural parameter found in (a) as $\phi(\theta)$.  Assume $p(\phi(\theta))\propto 1$.  Note that this is an improper prior distribution because it CANNOT integrate to 1 since the support of the natural parameter is $(-\infty,\infty)$.  Determine $p(\theta)$ using the change-of-variable formula. Note that $\phi(\theta)$ will be  equivalent to $h^{-1}$ from the notes! You know the distribution of $\phi$ and want to find this distribution of $\theta$ through the change of variable formula. You actually have fewer steps to complete than usual. Typically, we have $\theta(\phi)=h(\phi)$ and must solve for $\phi(\theta) = h^{-1}(\theta)$. In our case, we already know $h^{-1}(\theta)$. We don’t need to solve for h.

### Setup

For this let us refer to the change of variable formula from the notes.

$$
p_{\phi}(\phi) = p_{\theta}(h^{-1}(\phi)) \left| \frac{dh^{-1}(\phi)}{d\phi} \right|
$$

This is written to solve for $p_{\phi}(\phi)$ which we already know. Our goal is to get $p_{\theta}(\theta)$, so we need to rewrite this. Thankfully it's as simple as swapping the symbols!

$$
p_{\theta}(\theta) = p_{\phi}(h^{-1}(\theta)) \left| \frac{dh^{-1}(\theta)}{d\theta} \right|
$$

Now all we really need is the first derivative of our answer from part a.

### Solution

(i) Derivative

$$
\begin{align}
     \frac{dh^{-1}(\theta)}{d\theta} &= \frac{d}{d\theta} ln\left( \frac{\theta}{1-\theta} \right) \\
     &= \frac{1}{\theta-\theta^2} \\
     &= \frac{1}{\theta(1-\theta)} \\
     &= \theta^{-1}(1-\theta)^{-1}
\end{align}
$$

(ii) Plug it in

Here we'll take advantage of the fact that $p_{\phi}(h^{-1}(\theta)) \propto 1$. This means all that is left really is the derivative.

$$
\begin{align}
    p_{\theta}(\theta) &=  p_{\phi}(h^{-1}(\theta)) \left| \frac{dh^{-1}(\theta)}{d\theta} \right| \\
    &\propto 1 \cdot \left\vert \theta^{-1}(1-\theta)^{-1} \right\vert \\
    &= \theta^{-1}(1-\theta)^{-1}
\end{align}
$$

This appears to be the kernel of a $Beta(0,0)$ distribution, but those values for $\alpha$ and $\beta$ are invalid for the distribution. So it's definitely improper.

## (c)
Use the prior derived in (b) to derive the posterior distribution for a $\mathrm{Bin}(n,\theta)$ sampling distribution.  Is the resulting posterior distribution always proper?

Okay so now we have a binomial data distribution. 

$$
p(y\mid n,\theta) = {n \choose y} \theta^y (1-\theta)^{n-y} \; ; \; y=0,1,...,n \;;\; 0 \leq \theta \leq 1
$$

Using that info we have:

$$
\begin{align}
    p(\theta \mid y) &\propto p(y \mid \theta) p(\theta) \\
    &\propto {n \choose y} \theta^y (1-\theta)^{n-y} \theta^{-1}(1-\theta)^{-1} \\
    &\propto \theta^{y-1} (1-\theta)^{n-y-1}
\end{align}
$$

Which appears to be the kernel of a $Beta(y, n-y)$ distribution. However, this is only a proper distribution when $y>0$ and $n-y>0$. 

# Problem 5:

Assume $y\mid \theta \sim \mathrm{Bin}(n,\theta)$ and $\theta∼U(0,1)$.

## (a)

Determine the prior predictive distribution for $y$, i.e., $p(y)$.  Note: Do not leave your result in terms of a Beta function. Simplify, simplify, simplify! You will get a nice, neat solution for $p(y)$. Does this make sense, intuitively? 

Let's set some stuff up.

$$
\begin{align}
    p(y \mid \theta) &= {n \choose y} \theta^y (1-\theta)^{n-y} \\
    p(\theta) &= \frac{1}{1-0} \\
    &= 1
\end{align}
$$

We want to find $p(y)$, so we can use those two pieces to get there.

$$
\begin{align}
    p(y) &= \int p(y \mid \theta) p(\theta) d\theta \\
    &= \int {n \choose y} \theta^y (1-\theta)^{n-y} \cdot 1 d\theta \\
    &= {n \choose y} \int \theta^y (1-\theta)^{n-y} d\theta \\
\end{align}
$$

From here we can recognize that the integrand is the kernel of a beta distribution with $\alpha=y+1$ and $\beta=n-y+1$. Why this is important is we don't need to evaluate the integral directly. We know that it will evaluate to be the inverse of the normalizing constant! That is, it'll evaluate to the value such that if the normalizing constant was in the integrand that the integral would evaluate to 1.

The normalizing constant of the beta distribution is:

$$
\begin{align}
c &= \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)} \\
\frac{1}{c} &= \frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha + \beta)}
\end{align}
$$

So, back to the integral, making sure to subtitute in the appropriate values of $\alpha$ and $\beta$.

Also, as a small note. $\Gamma(x) = (x-1)!$.

$$
\begin{align}
    p(y) &= {n \choose y} \int \theta^y (1-\theta)^{n-y} d\theta \\
    &= {n \choose y} \frac{\Gamma(y+1)\Gamma(n-y+1)}{\Gamma(y+1+n-y+1)} \\
    &= \frac{n!}{y!(n-y)!} \cdot \frac{y!(n-y)!}{(n+1)!} \\
    &= \frac{n!}{(n+1)n!} \\
    &= (n + 1)^{-1}
\end{align}
$$

This does make sense intuitively. Our prior is a non-informative one. It doesn't tell us anything about which values of $y$ would be more likely than others. What we see in our result is that it isn't a function of $y$ and, as such, all of the values along the support are equally likely. 

## (b)

Determine the formula for the posterior variance for $\theta$.

For the posterior variance we first need the posterior. Thankfully we've done that work already in a way. Looking back at our computation for the marginal density of $y$ included everything we need for the posterior. 

$$
p(y) = \int p(y \mid \theta) p(\theta) d\theta \\
p(\theta \mid y) \propto  p(y \mid \theta) p(\theta)
$$

From this, we already know that the distribution of the posterior is a beta distribution with $\alpha=y+1$ and $\beta=n-y+1$. It works out really conveniently that way!

As for the variance, we can directly apply the variance of the beta distribution.

Let $X \sim Beta(\alpha, beta)$. $Var(X) = \frac{\alpha\beta}{(\alpha + \beta)^2 (\alpha + \beta + 1)}$

$$
\begin{align}
    Var(\theta \mid y) &= \frac{(y+1)(n-y+1)}{(y+1+n-y+1)^2(y+1+n-y+1 + 1)} \\
    &= \frac{(y+1)(n-y+1)}{(n+2)^2(n+3)} \\
\end{align}
$$

## (c)

Determine the prior variance for $\theta$. 

The prior is just a uniform distribution, so we can easily find the variance.

$$
Var(\theta) = \frac{(1-0)^2}{12} = \frac{1}{12}
$$

## (d)

**Problem**

Is the posterior variance for $\theta$ ever greater than the prior variance?  If so, provide a counter example. If not, then prove the posterior variance is always smaller than the prior variance.

**Answer**

For this problem we'll be sticking to the same prior and data distribution as the rest of problem 5. So a uniform prior and binomial data distribution. 

With this set up the posterior variance will never be larger than the prior variance. What's more, is that, the best case scenario is in the trivial situation with no data $y=n=0$. In that situation the variances match.

**Setup**

We'll be leveraging the posterior variance formula from the previous section:

$$
Var(\theta \mid y) = \frac{(y+1)(n-y+1)}{(n+2)^2(n+3)}
$$

And the prior variance

$$
Var(\theta) = \frac{1}{12}
$$

So our proof will be to verify that

$$
    Var(\theta \mid y) < Var(\theta), \; \forall\; n\geq1, \; 0\leq y\leq n
$$

**Special Case:** $n=y=0$

Before we start, let us entertain the trivial example mentioned before.

$$
\begin{align}
    Var(\theta \mid y) &= \frac{(y+1)(n-y+1)}{(n+2)^2(n+3)} \\
    &= \frac{1\cdot1}{2^2 \cdot 3} \\
    &= \frac{1}{12} \\
    Var(\theta \mid y) &= Var(\theta)
\end{align}
$$

This should make intuitive sense as, with no data, all of the variance will be contained within the prior. 

**Other Cases**

Let us now proceed with all cases where $n\geq1$.

When we expand out our posterior variance our inequality looks like

$$
\begin{align}
    \frac{yn-y^2+n+1}{n^3+7n^2+16n+12} &< \frac{1}{12} \\
    \frac{12(yn-y^2+n+1)}{n^3+7n^2+16n+12} &< 1 & \text{(mult by 12)} \\
\end{align}
$$

Looking into this, the numerators largest term is a quadratic, $y^2$. However, $0\leq y \leq n$, so $y$ becoming as large as possible results in the quadratic cancelling out. The bounds on $y$ prevent the numerator from ever becoming too large. However, the denominator will always increase as $n$ increases. On top of that, it is a cubic which will grow faster than the quadratic ever could.

Looking at the limits as both numbers approach infinity we get,

$$
\begin{align}
    \lim_{n,y\to\infty} \frac{12(yn-y^2+n+1)}{n^3+7n^2+16n+12} &< 1 \\
    0 &< 1
\end{align}
$$

This holds for all $n,y$. 

**Summary**

In summary, for $n=0$, the variance of the posterior matches the prior.

For situations where $n \geq 1$, the denominator greatly outpaces the numerator, ensuring that the posterior variance can never be greater than the prior variance.

# Problem 6

Assume $y\mid \theta\sim \mathrm{Bin}(n,\theta)$ and $\theta \sim \mathrm{Beta}(\alpha, \beta)$.  Is the posterior variance for $\theta$ ever greater than the prior variance?  If so, provide an example.

Yes it certainly can be in this situation. 

Let's look at an example. I came up with this example by throwing random numbers at the wall.

Let $\theta \sim \mathrm{Beta}(1, 11)$.

We'll have 3 trials $(n=3)$ and 3 successes $(y=3)$ for our binomial rv.

**Variance of Prior**

$$
\begin{align}
    Var(\theta) &= \frac{11\cdot1}{12^2 \cdot 13} \\
    &\approx 0.0059
\end{align}
$$

**Variance of Posterior**

Our posterior, $\theta \mid y$ will have a distribution $Beta(1+3, 11+3) = Beta(4,14)$.

$$
\begin{align}
    Var(\theta \mid y) &= \frac{4 \cdot 12}{16^2 \cdot 17} \\
    &\approx 0.011
\end{align}
$$

**Result**

As $0.011 > 0.0059$, the posterior variance can infact be larger than the prior variance. This is because our prior very much did not align with the data at all. $E[\theta]=1/12$ which is a far cry from our data success probability of $1$. This contradiction, combined with a small sample size, is what helped result in that increased posterior variance. 

# Problem 7

In general, when might the posterior variance be greater than the prior variance?  (Not for this specific example, but in general?)

In general, large conflicts between the prior assumptions and data can cause this. A very strongly informed prior matched with extreme outliers or contradicting data can throw the variance out of wack for the posterior distribution. It also depends on what distributions are used. Some distributions will force the posterior variance to be lower than the prior. We saw this in problem 5. Not all distributions will force this to be true, as we saw in problem 6.

# Problem 8

Use Bayes’ theorem to verify the formulas for $\mu_n$ and $\tau_n^2$ given in the notes when $y_1,\ldots,y_n \mid \theta ∼ N(\theta, \sigma^2)$ and $\theta \sim N(\mu_0, \tau_0^2)$, with $\sigma^2$ assumed known.  Hint:  complete the square!
