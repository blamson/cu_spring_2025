---
title: "Homework 3"
author: "Brady Lamson"
format: html
self-contained: true
---

```{r, include=FALSE, echo=FALSE, message=FALSE, warning=FALSE}
packages <- c("mvtnorm", "cubature", "ggplot2", "latex2exp", "glue")

for (pkg in packages) {
  if (!require(pkg, character.only = TRUE, quietly = TRUE)) {
    install.packages(pkg, repos = "https://cran.rstudio.com/")
    library(pkg, character.only = TRUE)
  }
}
```


# Problems 1-5

Data dist: negative binomial. $n=200$, $r=503-200=303$

$y\mid\theta \sim NBin(y=303, n=200, p=\theta)$

prior dist: $Beta(1.1, 1.5)$

$\theta \sim Beta(\alpha=1.1, \beta=1.5)$

## Problem 1

Create a graphic of the plotting the likelihood function and the prior density versus $\theta$. Recall that the likelihood function is the data density evaluated at the observed data values as a function of $\theta$. Make sure to scale the likelihood function so that its mode is similar to the mode of the prior. Make sure to provide a legend distinguishing the two functions.

```{r}
# Setup our table of prior and likelihood values
n_trials <- 503
n_succ <- 200
n_fail <- n_trials - n_succ

theta_support <- seq(0.001,0.999, length=1000)

prior <- dbeta(x=theta_support, shape1=1.1, shape2=1.5)

likelihood <- dnbinom(x=n_fail, size=n_succ, prob = theta_support)

df <- data.frame(theta_support, prior, likelihood)

# Scale the likelihood so both it and the prior are on a similar scale
df$likelihood_scaled <- df$likelihood / max(df$likelihood) * max(df$prior)
```

```{r}
line_size <- 1.2
ggplot(df, aes(x = theta_support)) +
    geom_line(aes(y = prior, color = "Prior"), linewidth = line_size) +
    geom_line(aes(y = likelihood_scaled, color = "Likelihood"), linewidth = line_size) +
    labs(
        x = TeX(r'($\theta$)'), 
        y = "Density", 
        title = "Prior and Scaled Data Distribution Along Support of Theta",
        color = ""
    ) +
    scale_x_continuous(breaks=seq(0,1,0.1)) +
    scale_color_discrete(labels = c(
        TeX(r'($p(\theta)$)'),TeX(r'($p(y | \theta)$)')
    )) +
    theme_minimal() +
    theme(legend.position = "top")
```

## Problem 2

```{r}
dpost <- function(theta, successes=200, failures=303, shape1=1.1, shape2=1.5, const = 1) {
    prior <- dbeta(x=theta, shape1=1.1, shape2=1.5)
    likelihood <- dnbinom(x=n_fail, size=n_succ, prob = theta)
    posterior <- prior * likelihood / const
    
    posterior
}

# plot(theta_support, dpost(theta=theta_support), type="l")
```

For fun let's take a look at how our posterior has shifted compared to the likelihood. 

```{r, echo=FALSE}
df$posterior <- df$prior * df$likelihood

line_size <- 1
ggplot(df, aes(x = theta_support)) +
    # geom_line(aes(y = posterior), linewidth = line_size, color="coral") +
    geom_line(aes(y = posterior, color = "Posterior"), linewidth = line_size) +
    geom_line(aes(y = likelihood, color = "Likelihood"), linewidth = line_size) +
    scale_x_continuous(breaks=seq(0,1,0.1)) +
    theme_minimal() +
    labs(
        x = TeX(r'($\theta$)'), 
        y = "Density", 
        #title = "Posterior Distribution"
        title = 'Comparing the Posterior and Likelihood'
    ) +
    theme(legend.position = "top")
```

What we can see is that it hasn't *shifted* much, but it definitely is more concentrated right around 0.4. 

Now that we've done that for no reason, let's get $\hat{\theta}_{MAP}$.

The notes have the MAP computation using the log posterior, let's try with and without it and see what we get.

```{r}
# determine MAP
map <- optimize(dpost, interval = c(0.3, 0.5), maximum = TRUE)$maximum

glue("The MAP estimate for theta is: {round(map,5)}")
```

```{r}
dpostlog <- function(theta, successes=200, failures=303, shape1=1.1, shape2=1.5, const = 1) {
    prior <- dbeta(x=theta, shape1=1.1, shape2=1.5, log = T)
    likelihood <- dnbinom(x=n_fail, size=n_succ, prob = theta, log = T)
    posterior <- exp(prior + likelihood - const)
    
    posterior
}

map <- optimize(dpostlog, interval = c(0.3, 0.5), maximum = TRUE)$maximum

glue("(Log Posterior) The MAP estimate for theta is: {round(map,5)}")
```

Same values. This MAP also matches the value of theta you get if you evaluate $E[\theta \mid y]$ by hand, we'll get back to that later.

## Problem 3

**Problem:** Determine the posterior mean and variance using deterministic methods (i.e., cubature methods). Note that you will need to determine the scaling constant associated with the marginal data density, $p(y)$.

### A

**Problem:** What is $p(y)$? Specifically, determine this constant numerically.

Let's break this down for my own sake.

Our current posterior that we're working with is *unnormalized*. That is, it won't integrate to 1. The value it integrates to is the inverse of the normalizing constant of our posterior! This is great because we can update our posterior with this constant to normalize it!

```{r}
const <- integrate(dpost, lower=0, upper=1)$value
glue("p(y) is: {round(const,5)}")
```

### B

**Problem:** What is the posterior mean?

Thankfully our `dpost` function already accounts for a constant, so we just need to plug that in. We multiply by $\theta$ because we're looking for the expected value. This is basic but easy to forget when we're looking at code.

$E[X] = \int x f(x) dx$

```{r}
mean_target <- function(theta, constant) {
    theta * dpost(theta=theta, const=constant)
}

mean_theta <- integrate(mean_target, lower=0, upper=1, constant=const)$value
glue("E[theta|y] is: {round(mean_theta,5)}")
```

### C

**Problem:** What is the posterior variance?

Here we'll use the classic:

$Var(X) = E[X^2] - E[X]^2$

We already have the latter part, for the former we update our function to square theta. 

```{r}
mean_target_squared <- function(theta, constant) {
    theta^2 * dpost(theta=theta, const=constant)
}

mean_theta_squared <- integrate(mean_target_squared, lower=0, upper=1, constant=const)$value
var_theta <- mean_theta_squared - mean_theta^2
glue("Var[theta|y] is: {round(var_theta,5)}")
```

## Problem 4

So, according to the normal approximation from the notes:

$$\theta \mid y \sim N(\hat{\theta}, I(\hat{\theta})^{-1})$$

Where $\hat{\theta} = \hat{\theta}_{MAP}$ in our case and 

$$I(\hat{\theta}) = -\frac{d^2}{d\hat{\theta}^2} \ln(p(y \mid \theta))$$

So we already know the mean of our distribution from our original MAP estimate. **The mean is** $\approx 0.397$.

For the variance we'll just need to sort out the second derivative of the log likelihood and evaluate it at our MAP estimator.

$$
\begin{align}
    \ln(p(y \mid r, \hat{\theta})) &= \ln \left( {r+y-1 \choose y} \theta^r (1-\theta)^y \right) \\
    &= \ln {r+y-1 \choose y} + r \ln(\theta) + y\ln(1-\theta) \\
    \frac{d}{d\hat{\theta}} \ln(p(y \mid r, \hat{\theta})) &= r\theta^{-1} - y(1-\theta)^{-1} \\
    \frac{d^2}{d\hat{\theta}^2} \ln(p(y \mid r, \hat{\theta})) &= -r\theta^{-2} - y(1-\theta)^{-2} \\
    \frac{d^2}{d\hat{\theta}^2} \ln(p(y=303 \mid r=200, \hat{\theta})) &= -200\theta^{-2} - 303(1-\theta)^{-2} \\
    I(\hat{\theta}) &= 200\theta^{-2} + 303(1-\theta)^{-2} \\
\end{align}
$$

From here we just need the inverse of this evaluated at our MAP estimate.

$Var(\theta \mid y) \approx (200 \cdot 0.397^{-2} + 303(1-0.397)^{-2})^{-1}$

```{r}
observed_inf <- n_succ * map^(-2) + n_fail * (1-map)^(-2)
norm_approx_var <- observed_inf^(-1)
glue("The normal approximated variance is {round(norm_approx_var, 5)}")
```

Which matches the posterior variance we calculated earlier using cubature methods.

## Problem 5

Our true posterior is a $Beta(201.1, 304.5)$ distribution. 

```{r}
# theta_support <- seq(0.001,0.999, length=1000)
# Lower count of theta for dashed line and zoom in
theta_support <- seq(0.2,0.6, length=100)
true_post <- dbeta(x=theta_support, shape1=201.1, shape2=304.5)
norm_approx <- dnorm(x=theta_support, mean=map, sd=sqrt(norm_approx_var))
df <- data.frame(theta_support, true_post, norm_approx)

line_size <- 1.2
ggplot(df, aes(x = theta_support)) +
    geom_line(aes(y = true_post, color = "True Posterior"), linewidth = line_size) +
    geom_line(aes(y = norm_approx, color = "Normal Approximation"), linewidth = line_size, linetype="dashed") +
    labs(
        x = TeX(r'($\theta$)'), 
        y = "Density", 
        title = "True Posterior compared to the Normal Approximation",
        color = ""
    ) +
    scale_x_continuous(breaks=seq(0,1,0.1)) +
    theme_minimal() +
    theme(legend.position = "top")
```

They match extremely well. Even with a variety of line widths it's difficult to see where any discrepancies even are. As such I made the sequence of $\theta$ smaller and used a dashed line for the normal approximation so we can see the overlap a bit better.