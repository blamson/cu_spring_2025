---
title: "Homework 4 - Computation (Stochastic)"
format: html
self-contained: true
---

```{r, include=FALSE, echo=FALSE, message=FALSE, warning=FALSE}
packages <- c("mvtnorm", "ggplot2", "latex2exp", "glue", "autoimage", "remotes")

for (pkg in packages) {
  if (!require(pkg, character.only = TRUE, quietly = TRUE)) {
    install.packages(pkg, repos = "https://cran.rstudio.com/")
    library(pkg, character.only = TRUE)
  }
}

# see if bayesutils package is available
if (!require("bayesutils", quietly = TRUE)) {
  # if not, then install package
  remotes::install_github("jfrench/bayesutils")
  library(bayesutils)
}

options(scipen = 20) # For fixing scientific notation nonsense
```

# Problem 1

Voter turnout has been a popular talking point in recent elections. To get a sense of voter turnout in Colorado, a national polling company contacted randomly selected registered Colorado voters until they reached 200 persons who said that they had voted. 303 persons said they had not voted, for a total sample size of 503. The response $y$ is the number of failures before getting the 200th registered voter who had voted. Thus, $y$ can be modeled as a negative binomial distribution with $n$ successes and probability of success $\theta$, where $\theta$ is the probability a registered voter stating they voted. Thus, we can describe the data distribution as $y \mid n, \theta \sim \mathrm{Neg}\text{-}\mathrm{Bin}(n,\theta)$ with
$$p(y\mid n,\theta)=\frac{\Gamma(y+n)}{\Gamma(n)y!} \theta^n (1-\theta)^yI_{\{0,1,2\ldots\}}(y).$$
Based on what you’ve heard, you believe that the true proportion of registered voters that actually vote is less than 50%, but you’re not super confident about this. Thus, you chose a Beta(1.1, 1.5) prior distribution for $\theta$.

Let $q(\theta \mid y)=p(y\mid \theta)p(\theta)$.

Implement a rejection sampler for the posterior distribution. 

## (a)

Determine a relevant proposal distribution. What proposal distribution will you use? You are NOT allowed to use a uniform distribution. Why did you choose this one?

**Solution**

My first instinct was to use a normal centered around $0.4$, but I forgot that the normal has a different support from our posterior. So instead I just went with a beta. It feels obvious so I didn't want to use it, but it's obviously a very strong candidate for a bound as we can match the shape as well as we want. 

I could've derived some good values for the parameters but I thought it'd be more fun to just throw values at the wall until something stuck! I'm looking for a bounding that is mostly close but has enough room to see a good number of rejections. It'd be no fun if it was too perfect of a bound. 

## (b)

Determine a scale constant $M$ to create an appropriate bounding function $Mg(\theta)$. Create a plot of $Mg(\theta)$ and $q(\theta \mid y)$ versus $\theta$, making sure to clearly distinguish the two functions.

**Solution**

Let's start with much the same setup as before. 

```{r}
#| code-fold: true
#| code-summary: Setting up our priors, posterior, etc.
# Setup our table of prior and likelihood values
n_trials <- 503
n_succ <- 200
n_fail <- n_trials - n_succ

# Basic data table setup like in hw3
theta_support <- seq(0.001,0.999, length=1000)
prior <- dbeta(x=theta_support, shape1=1.1, shape2=1.5)
likelihood <- dnbinom(x=n_fail, size=n_succ, prob = theta_support)
df <- data.frame(theta_support, prior, likelihood)

# Create a column for our un-normalized posterior along the support of theta
q_post <- function(theta, successes=200, failures=303, shape1=1.1, shape2=1.5, const = 1) {
    prior <- dbeta(x=theta, shape1=1.1, shape2=1.5)
    likelihood <- dnbinom(x=n_fail, size=n_succ, prob = theta)
    posterior <- prior * likelihood / const
    
    posterior
}

df$posterior <- q_post(theta_support)
```

Below is the plot, we add our scaled bound to the data frame here as well. Makes it easier to test various parameter and scale values in the same block as the plot.

```{r}
#| code-fold: true
#| code-summary: Plot code

# This chunk sets up our proposal distribution
# The function is literally just the beta I'd like to just have it set for convenience.
M <- 0.0045
g_alpha <- 10.5
g_beta <- 15.5

# Data columns for plots
df$proposal_dist <- dbeta(theta_support, shape1 = g_alpha, shape2 = g_beta)
df$bound <- df$proposal_dist * M

line_size <- 1.2
ggplot(df, aes(x = theta_support)) +
    geom_line(aes(y = posterior, color = "Posterior"), linewidth = line_size) +
    geom_line(aes(y = bound, color = "Bounding Function"), linewidth = line_size) +
    labs(
        x = TeX(r'($\theta$)'), 
        y = "Density", 
        title = "Unnormalized Posterior and Bounding Function",
        color = ""
    ) +
    scale_x_continuous(breaks=seq(0,1,0.1)) +
    scale_color_discrete(labels = c(
        TeX(r'($g(\theta)M$)'), TeX(r'($p(\theta | y)$)')
    )) +
    theme_minimal() +
    theme(legend.position = "right")
```

As you can see we ended up opting for $M=0.005$. This scalar was chosen arbitrarily, it seems to give a decent enough bound to cover our posterior here. It also gives enough wiggle room in the middle that we can still see the rejection sampling work its magic! As for $\alpha$ and $\beta$ I opted for $10.5$ and $15.5$ respectively as it gets us around the right expected value.

## (c)

Run the rejection sampler so that 100,000 samples are accepted. Then create a plot showing the true posterior density (the normalizing constant was determined in the previous homework) versus the approximate density coming from the rejection sampler, making sure to clearly distinguish the two densities.

**Some Tinkering**

This parts just for fun, feel free to skip if you just want to see the solution below.

First let's test for a smaller number of samples so we can plot the actual points. This will help us not blindly rely on the notes and actually verify things are working a bit.

```{r}
#| code-fold: true
#| code-summary: Code for demonstration of rejection sampling

set.seed(100)
theta_results <- c()
u_results <- c()
outcome <- c()
for(i in 1:1000) {
    theta_star <- rbeta(1, shape1 = g_alpha, shape2 = g_beta)
    u_star <- runif(1, min=0, max=M*dbeta(theta_star, shape1 = g_alpha, shape2 = g_beta))
    
    # Append results to vectors
    outcome <- c(outcome, u_star <= q_post(theta_star))
    u_results <- c(u_results, u_star)
    theta_results <- c(theta_results, theta_star)
}
results <- data.frame(theta = theta_results, u = u_results, outcome = outcome)

# Plot results under previous plot
line_size <- 1
ggplot(df, aes(x = theta_support)) +
    geom_line(aes(y = posterior, color = "Posterior"), linewidth = line_size) +
    geom_line(aes(y = bound, color = "Bounding Function"), linewidth = line_size) +
    geom_point(aes(results$theta, results$u, color=results$outcome), size=0.5) +
    labs(
        x = TeX(r'($\theta$)'), 
        y = "Density", 
        title = "Rejection Sampler Example Results",
        color = ""
    ) +
    scale_x_continuous(breaks=seq(0,1,0.1)) +
    theme_minimal() +
    theme(legend.position = "right") +
    scale_color_discrete(labels = c(
        TeX(r'($g(\theta)M$)'), "Rejected", TeX(r'($p(\theta | y)$)'), "Accepted"
    ))
```

```{r, echo=FALSE}
prop_accepted <- sum(results$outcome) / nrow(results)
expected_trials <- round(100000 / prop_accepted)
cat(
    "Proportion of samples kept: ", prop_accepted, "\n",
    "Expected trials for 100k accepted samples: ", expected_trials, "\n",
    sep=""
)
```

Okay neat. Definitely some inefficiency here but nothing that will keep me up all night! Let's collect our samples!

**Solution**

```{r}
#| code-fold: true
#| code-summary: Rejection sampling code

set.seed(100)

B <- 100000

# Preallocate vectors to store only accepted samples
theta_results <- numeric(B)
u_results <- numeric(B)

# Setup some counters for tracking progress
accepted <- 0 
total_samples <- 0  

while (accepted < B) {
    total_samples <- total_samples + 1
    
    # Run the sample
    theta_star <- rbeta(1, shape1 = g_alpha, shape2 = g_beta)
    u_star <- runif(1, min = 0, max = M * dbeta(theta_star, shape1 = g_alpha, shape2 = g_beta))
    
    # Store accepted sampling results
    if (u_star <= q_post(theta_star)) {
        accepted <- accepted + 1
        theta_results[accepted] <- theta_star
        u_results[accepted] <- u_star
    }
    
    # Log progress every B iterations due to expected 0.20 acceptance rate
    if (total_samples %% B == 0) {
        cat(
            "---\n",
            "Samples Accepted: ", accepted, "\n",
            "Total Iterations: ", total_samples, "\n",
            "Proportion Accepted: ", round(accepted / total_samples, 5), "\n",
            sep=""
        )
    }
}

results <- data.frame(theta = theta_results, u = u_results)
```

```{r, echo=FALSE}
cat(
    "---FINAL RESULTS---\n",
    "Samples Accepted: ", accepted, "\n",
    "Total Iterations: ", total_samples, "\n",
    "Proportion Accepted: ", round(accepted / total_samples, 5), "\n",
    sep=""
)
```

```{r}
#| code-fold: true
#| code-summary: More plotting code!

true_post <- dbeta(theta_support, shape1=201.1, shape2=304.5)
true_post <- data.frame(theta = theta_support, density = true_post)

line_size <- 1.2
ggplot(results, aes(x=theta)) +
    geom_density(aes(color="Sampling Density"), linewidth=line_size) +
    geom_line(data=true_post, aes(x=theta, y=density, color="True Posterior"), linetype="dashed", linewidth=line_size) +
    labs(
        x = TeX(r'($\theta$)'), 
        y = "Density", 
        title = "Comparing Densities of Sampling Algorithm to True Posterior",
        color = ""
    ) +
    scale_x_continuous(breaks=seq(0,1,0.1)) +
    theme_minimal() +
    theme(legend.position = "top")
```


# Problem 2

Suppose that the unnormalized bivariate posterior distribution for parameters $\theta_1$ and $\theta_2$ is $q(\theta_1,\theta_2 \mid y)=\theta_1^2 \exp(-\theta_1 \theta_2^2 -\theta_2^2 + 2\theta_2 - 4\theta_1)I_{(0,\infty)}(\theta_1)$

## (a)

Derive the full conditional distribution for $\theta_1$.

**Solution**

## (b)

Derive the full conditional distribution for $\theta_2$.

**Solution**

## (c)

Run a Gibbs sampler for $\theta_1$ and $\theta_2$ for 100,000 cycles.  Plot the estimated density of $p(\theta_1\mid y)$ and $p(\theta_2 \mid y)$ using these samples.

**Solution**

Using our answers from part a and b where:

$$
\theta_1 | \theta_2,y \sim Gamma\left(3, \frac{1}{\theta_2^2 + 4}\right) \\
\theta_2 | \theta_1,y \sim N\left( \frac{1}{\theta_1+1}, \frac{1}{2(\theta_1+1)} \right)
$$

```{r}
#| code-fold: true
#| code-summary: Gibbs Sampler Code
gibbs <- function(B, theta) {
    # Function takes in 
    # - B: which is an integer to specify total iterations
    # - theta: a 2 length vector of parameter values
    
    # We want a check for theta_1 here. An indicator function isn't sufficient
    # as we aren't calculating density here. We need to throw an error and exit.
    if (theta[1] < 0) {
        stop(
            paste("Invalid parameter, theta_1 must be positive. Value: ", theta[1])
        )
    }
    
    # Create matrix for samples, ncol=2 due to 2 parameters
    theta_sims <- matrix(0, nrow=B+1, ncol=2)
    # Set first row to be starting values of theta
    theta_sims[1,] <- theta
    
    for (i in 2:(B+1)) {
        # Simulate from full conditional distribution for theta1
        # Note that our parameterization of the gamma uses the scale parameter, not rate
        alpha <- 3
        beta <- 1/(theta[2]^2 + 4)
        theta[1] <- rgamma(1, shape=alpha, scale=beta)
        
        # Determine full conditional mean for theta2
        mu_2 <- 1 / (theta[1] + 1)
        sigma_2 <- sqrt(1 / ( 2* (theta[1] + 1) ))
        theta[2] <- rnorm(1, mu_2, sigma_2)
        
        # Save sample
        theta_sims[i, ] <- theta
    }
    
    return(theta_sims)
}
```

Let's run a sanity check here.

```{r}
#| code-fold: true
#| code-summary: Trial run
#| 
theta <- c(0,0)
B <- 1000
chain1 <- gibbs(B, theta)

plot(chain1, pch = ".",
     xlab = expression(theta[1]),
     ylab = expression(theta[2]))
title("Samples from Gibbs sampler")
```


```{r}
#| code-fold: true
#| code-summary: Full Run

theta <- c(0,0)
B <- 100000
full_chain <- gibbs(B, theta)
gibbs_df <- as.data.frame(full_chain)
ggplot(gibbs_df) +
    geom_density(aes(x=V1, color="Theta 1"), linewidth=line_size) +
    geom_density(aes(x=V2, color="Theta 2"), linewidth=line_size) +
    theme_minimal() +
    labs(
        x = TeX(r'($\theta$)'), 
        y = "Density", 
        title = "Gibbs Sampling: Densities of Theta",
        color = ""
    ) +
    scale_color_discrete(labels = c(
        TeX(r'($p(\theta_1 | y)$)'), TeX(r'($p(theta_2 | y)$)')
    )) +
    scale_x_continuous(breaks=seq(-2,4,1))
```


## (d)

Determine the posterior mean, median, variance, and 95% central posterior interval for each parameter.

**Solution**

```{r}
#| code-fold: true
#| code-summary: Summary results of theta
quants <- c(0.025, 0.975)
gibbs_summary <- function(theta_samples, name) {
    theta_quants <- quantile(theta_samples, quants) |> round(5)
    cat(
        "--- Theta ", name, " Summary---\n",
        "Mean: ", round(mean(theta_samples), 5), "\n",
        "Median: ", round(median(theta_samples), 5), "\n",
        "Variance: ", round(var(theta_samples), 5), "\n",
        "95% Central Interval: [", theta_quants[1], ", ", theta_quants[2], "]\n\n",
        sep=""
    )
}
for (i in 1:2) {gibbs_summary(gibbs_df[,i], i)}

```


## (e)

Construct a plot showing the movement of the sample values for the first 100 cycles of the Gibbs sampler.  Label the axes appropriately.

**Solution**

```{r}
#| code-fold: true
#| code-summary: MCMC Path Plot
plot_mcmc_path(
  full_chain,
  ncycles = 100,
  xlim = c(-0.25, 2.5),
  ylim = c(-1.5, 2),
  xlab = expression(theta[1]),
  ylab = expression(theta[2]),
  main = "First 100 cycles of the chain"
)
```

For fun let's look at the last 100 cycles.

```{r}
#| code-fold: true
#| code-summary: MCMC Path Plot (Last 100 cycles)
plot_mcmc_path(
  full_chain[99000:100001, ],
  ncycles = 100,
  xlim = c(-0.25, 2.5),
  ylim = c(-1.5, 2),
  xlab = expression(theta[1]),
  ylab = expression(theta[2]),
  main = "Last 100 cycles of the chain"
)
```

# Problem 3

Consider the data in `coal.txt`, which contains counts for the number of coal mine disasters over a 112-year period (1851 to 1962) in the United Kingdom. The data have relatively high disaster counts in the early era, and relatively low counts in the later era. When did technology improvements and safety practices have an actual effect on the rate of serious accidents?

Assume that $y_1,y_2,\ldots,y_K\mid \lambda, K \stackrel{i.i.d.}{\sim} \mathrm{Poisson}(\lambda)$ and $y_{K+1},y_{K+2},\ldots,y_{112}\mid \phi, k \stackrel{i.i.d.}{\sim} \mathrm{Poisson}(\phi)$. 
In this model, $K$ is the year that the “change” occurred when technology and/or safety improvements changed the pattern of coal mine disasters. Our interest lies in finding the posterior distributions for $\lambda$, $\phi$, and $K$. Assume a Gamma(4, 1) prior distribution for $\lambda$, a Gamma(1, 2) prior distribution for $\phi$, and a discrete uniform prior distribution on $[1,2, \ldots,112]$ for $K$, i.e., $p_K(k)=1/112$ for $k\in\{1,2,\ldots,112\}$.  

## (a)

Determine the full conditional distribution for $\lambda$. It is a common distribution. Hint: The distribution will depend on a sum of the counts that depends on the value of $K$.

**Solution**

## (b)

Determine the full conditional distribution for $\phi$.  It is a common distribution. Hint: The distribution will depend on a sum of the counts that depends on the value of $K$.

**Solution**

## (c)

Show that the full conditional distribution for $K$ is
$$p(K\mid\lambda,\phi,y)=\frac{\exp\left(K(\phi-\lambda)\right)\left(\frac{\lambda}{\phi}\right)^{\sum_{i=1}^K y_i}}{\sum_{K=1}^{112} \exp\left(K(\phi-\lambda)\right)\left(\frac{\lambda}{\phi}\right)^{\sum_{i=1}^K y_i}}.$$

**Solution**

## (d)

Run a 100,000 cycle Gibbs sampler using the distributions determined in parts (a) through (c) to approximate the posterior distribution of the three parameters.  Plot the estimated density/mass function of each parameter.

**Solution**

## (e)

Determine the posterior mean, median, variance, and 95% central posterior interval for each parameter.

**Solution**

## (f)

Let $m$ be the median of the posterior distribution for $K$.  Create a time series plot of the counts where the first $m$ years are shown in blue, while the remaining years are shown in orange.  This distinguishes the years where we have a $\mathrm{Poisson}(\lambda)$ distribution from a $\mathrm{Poisson}(\phi)$ distribution.  

**Solution**