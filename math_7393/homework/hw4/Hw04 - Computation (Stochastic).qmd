---
title: "Homework 4 - Computation (Stochastic)"
format: html
self-contained: true
author: Brady Lamson
date: 3/7/2025
---

```{r, include=FALSE, echo=FALSE, message=FALSE, warning=FALSE}
packages <- c("mvtnorm", "ggplot2", "latex2exp", "glue", "remotes", "dplyr")

for (pkg in packages) {
  if (!require(pkg, character.only = TRUE, quietly = TRUE)) {
    install.packages(pkg, repos = "https://cran.rstudio.com/")
    library(pkg, character.only = TRUE)
  }
}

# see if bayesutils package is available
if (!require("bayesutils", quietly = TRUE)) {
  # if not, then install package
  remotes::install_github("jfrench/bayesutils")
  library(bayesutils)
}

options(scipen = 20) # For fixing scientific notation nonsense
```

# NOTE

I should mention all of my code is included in this html file, I just have it toggleable so it isn't constantly cluttering up the page. You'll see various toggles as you scroll through the homework! Let me know if this is more of a hassle than just showing all the code outright. I'd never use these toggles before so I thought it'd be a fun exercise. 

# Problem 1

Voter turnout has been a popular talking point in recent elections. To get a sense of voter turnout in Colorado, a national polling company contacted randomly selected registered Colorado voters until they reached 200 persons who said that they had voted. 303 persons said they had not voted, for a total sample size of 503. The response $y$ is the number of failures before getting the 200th registered voter who had voted. Thus, $y$ can be modeled as a negative binomial distribution with $n$ successes and probability of success $\theta$, where $\theta$ is the probability a registered voter stating they voted. Thus, we can describe the data distribution as $y \mid n, \theta \sim \mathrm{Neg}\text{-}\mathrm{Bin}(n,\theta)$ with
$$p(y\mid n,\theta)=\frac{\Gamma(y+n)}{\Gamma(n)y!} \theta^n (1-\theta)^yI_{\{0,1,2\ldots\}}(y).$$
Based on what you’ve heard, you believe that the true proportion of registered voters that actually vote is less than 50%, but you’re not super confident about this. Thus, you chose a Beta(1.1, 1.5) prior distribution for $\theta$.

Let $q(\theta \mid y)=p(y\mid \theta)p(\theta)$.

Implement a rejection sampler for the posterior distribution. 

## (a)

Determine a relevant proposal distribution. What proposal distribution will you use? You are NOT allowed to use a uniform distribution. Why did you choose this one?

**Solution**

My first instinct was to use a normal centered around $0.4$, but I forgot that the normal has a different support from our posterior. So instead I just went with a beta. It feels obvious so I didn't want to use it, but it's obviously a very strong candidate for a bound as we can match the shape as well as we want. 

I could've derived some good values for the parameters but I thought it'd be more fun to just throw values at the wall until something stuck! I'm looking for a bounding that is mostly close but has enough room to see a good number of rejections. It'd be no fun if it was too perfect of a bound. 

## (b)

Determine a scale constant $M$ to create an appropriate bounding function $Mg(\theta)$. Create a plot of $Mg(\theta)$ and $q(\theta \mid y)$ versus $\theta$, making sure to clearly distinguish the two functions.

**Solution**

Let's start with much the same setup as before. 

```{r}
#| code-fold: true
#| code-summary: Setting up our priors, posterior, etc.
# Setup our table of prior and likelihood values
n_trials <- 503
n_succ <- 200
n_fail <- n_trials - n_succ

# Basic data table setup like in hw3
theta_support <- seq(0.001,0.999, length=1000)
prior <- dbeta(x=theta_support, shape1=1.1, shape2=1.5)
likelihood <- dnbinom(x=n_fail, size=n_succ, prob = theta_support)
df <- data.frame(theta_support, prior, likelihood)

# Create a column for our un-normalized posterior along the support of theta
q_post <- function(theta, successes=200, failures=303, shape1=1.1, shape2=1.5, const = 1) {
    prior <- dbeta(x=theta, shape1=1.1, shape2=1.5)
    likelihood <- dnbinom(x=n_fail, size=n_succ, prob = theta)
    posterior <- prior * likelihood / const
    
    posterior
}

df$posterior <- q_post(theta_support)
```

Below is the plot, we add our scaled bound to the data frame here as well. Makes it easier to test various parameter and scale values in the same block as the plot.

```{r}
#| code-fold: true
#| code-summary: Plot code

# This chunk sets up our proposal distribution
# The function is literally just the beta I'd like to just have it set for convenience.
M <- 0.0045
g_alpha <- 10.5
g_beta <- 15.5

# Data columns for plots
df$proposal_dist <- dbeta(theta_support, shape1 = g_alpha, shape2 = g_beta)
df$bound <- df$proposal_dist * M

line_size <- 1.2
ggplot(df, aes(x = theta_support)) +
    geom_line(aes(y = posterior, color = "Posterior"), linewidth = line_size) +
    geom_line(aes(y = bound, color = "Bounding Function"), linewidth = line_size) +
    labs(
        x = TeX(r'($\theta$)'), 
        y = "Density", 
        title = "Unnormalized Posterior and Bounding Function",
        color = ""
    ) +
    scale_x_continuous(breaks=seq(0,1,0.1)) +
    scale_color_discrete(labels = c(
        TeX(r'($g(\theta)M$)'), TeX(r'($p(\theta | y)$)')
    )) +
    theme_minimal() +
    theme(legend.position = "right")
```

As you can see we ended up opting for $M=0.005$. This scalar was chosen arbitrarily, it seems to give a decent enough bound to cover our posterior here. It also gives enough wiggle room in the middle that we can still see the rejection sampling work its magic! As for $\alpha$ and $\beta$ I opted for $10.5$ and $15.5$ respectively as it gets us around the right expected value.

## (c)

Run the rejection sampler so that 100,000 samples are accepted. Then create a plot showing the true posterior density (the normalizing constant was determined in the previous homework) versus the approximate density coming from the rejection sampler, making sure to clearly distinguish the two densities.

**Some Tinkering**

This parts just for fun, feel free to skip if you just want to see the solution below.

First let's test for a smaller number of samples so we can plot the actual points. This will help us not blindly rely on the notes and actually verify things are working a bit.

```{r}
#| code-fold: true
#| code-summary: Code for demonstration of rejection sampling

set.seed(100)
theta_results <- c()
u_results <- c()
outcome <- c()
for(i in 1:1000) {
    theta_star <- rbeta(1, shape1 = g_alpha, shape2 = g_beta)
    u_star <- runif(1, min=0, max=M*dbeta(theta_star, shape1 = g_alpha, shape2 = g_beta))
    
    # Append results to vectors
    outcome <- c(outcome, u_star <= q_post(theta_star))
    u_results <- c(u_results, u_star)
    theta_results <- c(theta_results, theta_star)
}
results <- data.frame(theta = theta_results, u = u_results, outcome = outcome)

# Plot results under previous plot
line_size <- 1
ggplot(df, aes(x = theta_support)) +
    geom_line(aes(y = posterior, color = "Posterior"), linewidth = line_size) +
    geom_line(aes(y = bound, color = "Bounding Function"), linewidth = line_size) +
    geom_point(aes(results$theta, results$u, color=results$outcome), size=0.5) +
    labs(
        x = TeX(r'($\theta$)'), 
        y = "Density", 
        title = "Rejection Sampler Example Results",
        color = ""
    ) +
    scale_x_continuous(breaks=seq(0,1,0.1)) +
    theme_minimal() +
    theme(legend.position = "right") +
    scale_color_discrete(labels = c(
        TeX(r'($g(\theta)M$)'), "Rejected", TeX(r'($p(\theta | y)$)'), "Accepted"
    ))
```

```{r, echo=FALSE}
prop_accepted <- sum(results$outcome) / nrow(results)
expected_trials <- round(100000 / prop_accepted)
cat(
    "Proportion of samples kept: ", prop_accepted, "\n",
    "Expected trials for 100k accepted samples: ", expected_trials, "\n",
    sep=""
)
```

Okay neat. Definitely some inefficiency here but nothing that will keep me up all night! Let's collect our samples!

**Solution**

```{r}
#| code-fold: true
#| code-summary: Rejection sampling code

set.seed(100)

B <- 100000

# Preallocate vectors to store only accepted samples
theta_results <- numeric(B)
u_results <- numeric(B)

# Setup some counters for tracking progress
accepted <- 0 
total_samples <- 0  

while (accepted < B) {
    total_samples <- total_samples + 1
    
    # Run the sample
    theta_star <- rbeta(1, shape1 = g_alpha, shape2 = g_beta)
    u_star <- runif(1, min = 0, max = M * dbeta(theta_star, shape1 = g_alpha, shape2 = g_beta))
    
    # Store accepted sampling results
    if (u_star <= q_post(theta_star)) {
        accepted <- accepted + 1
        theta_results[accepted] <- theta_star
        u_results[accepted] <- u_star
    }
    
    # Log progress every B iterations due to expected 0.20 acceptance rate
    if (total_samples %% B == 0) {
        cat(
            "---\n",
            "Samples Accepted: ", accepted, "\n",
            "Total Iterations: ", total_samples, "\n",
            "Proportion Accepted: ", round(accepted / total_samples, 5), "\n",
            sep=""
        )
    }
}

results <- data.frame(theta = theta_results, u = u_results)
```

```{r, echo=FALSE}
cat(
    "---FINAL RESULTS---\n",
    "Samples Accepted: ", accepted, "\n",
    "Total Iterations: ", total_samples, "\n",
    "Proportion Accepted: ", round(accepted / total_samples, 5), "\n",
    sep=""
)
```

```{r}
#| code-fold: true
#| code-summary: More plotting code!

true_post <- dbeta(theta_support, shape1=201.1, shape2=304.5)
true_post <- data.frame(theta = theta_support, density = true_post)

line_size <- 1.2
ggplot(results, aes(x=theta)) +
    geom_density(aes(color="Sampling Density"), linewidth=line_size) +
    geom_line(data=true_post, aes(x=theta, y=density, color="True Posterior"), linetype="dashed", linewidth=line_size) +
    labs(
        x = TeX(r'($\theta$)'), 
        y = "Density", 
        title = "Comparing Densities of Sampling Algorithm to True Posterior",
        color = ""
    ) +
    scale_x_continuous(breaks=seq(0,1,0.1)) +
    theme_minimal() +
    theme(legend.position = "top")
```


# Problem 2

Suppose that the unnormalized bivariate posterior distribution for parameters $\theta_1$ and $\theta_2$ is $q(\theta_1,\theta_2 \mid y)=\theta_1^2 \exp(-\theta_1 \theta_2^2 -\theta_2^2 + 2\theta_2 - 4\theta_1)I_{(0,\infty)}(\theta_1)$

## (a)

Derive the full conditional distribution for $\theta_1$.

**Solution**

We're given $q(\theta_1, \theta_2 \mid y)$ and we want $q(\theta_1 \mid \theta_2, y)$. Here we'll treat $\theta_2$ as a constant and focus on the parts of the posterior that involves $\theta_1$.

$$
\begin{align}
    q(\theta_1,\theta_2 \mid y) &= \theta_1^2 \exp(-\theta_1 \theta_2^2 -\theta_2^2 + 2\theta_2 - 4\theta_1)I_{(0,\infty)}(\theta_1) \\
    q(\theta_1 \mid \theta_2, y) &\propto \theta_1^2 \exp(-\theta_1\theta_2 - 4\theta_1)I_{(0,\infty)}(\theta_1) \\
    &\propto \theta_1^{3-1} \exp(-\theta_1(\theta_2^2 + 4))I_{(0,\infty)}(\theta_1)
\end{align}
$$

This is the kernel of a gamma distribution with $\alpha=3, \beta=(\theta_2^2 + 4)^{-1}$. 

Therefore, $\theta_1 \mid \theta_2, y \sim gamma(\alpha=3, \beta=(\theta_2^2 + 4)^{-1})$

## (b)

Derive the full conditional distribution for $\theta_2$.

Same plan, this ones a bit more involved.

**Solution**

$$
\begin{align}
    q(\theta_1,\theta_2 \mid y) &= \theta_1^2 \exp(-\theta_1 \theta_2^2 -\theta_2^2 + 2\theta_2 - 4\theta_1)I_{(0,\infty)}(\theta_1) \\
    q(\theta_2 \mid \theta_1, y) &\propto \exp(-\theta_1 \theta_2^2 -\theta_2^2 + 2\theta_2) \\
    &\propto \exp(\theta_2^2(-\theta_1 - 1) + 2\theta_2)
\end{align}
$$

Note: We have a quadratic form here of $(a\theta_2^2 + 2\theta_2)$ with $a=(-\theta_1 - 1)$. This is the big hint that we should be building towards the kernel of a normal distribution. Let's look at the normal kernel real quick.

$$
    q(x) \propto \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)
$$

We thought we were done completing the square and we get sucked right back in. Let's focus on the values in the exponent for now.

$$
\begin{align}
    a\theta_2^2 + 2\theta_2 &= a\left(\theta_2^2 + \frac{2}{a}\theta_2\right) \\
    &= a\left(\theta_2^2 + \frac{2}{a}\theta_2 + \frac{1}{a^2} - \frac{1}{a^2} \right) & \text{(Complete the square)} \\
    &= a\left( \left(\theta_2 + \frac{1}{a}\right)^2 - \frac{1}{a^2} \right) \\
    &= a\left(\theta_2 + \frac{1}{a}\right)^2 - \frac{1}{a} & \text{(Distribute }a) \\
    &=  (-\theta_1 - 1)\left(\theta_2 + \frac{1}{(-\theta_1 - 1)}\right)^2 - \frac{1}{(-\theta_1 - 1)} & \text{(Substitute }a) \\
    q(\theta_2 \mid \theta_1, y) &\propto \exp\left( (-\theta_1 - 1)\left(\theta_2 + \frac{1}{(-\theta_1 - 1)}\right)^2 - \frac{1}{(-\theta_1 - 1)} \right) & \text{(Plug back into exponent) } \\
     &\propto \exp\left( -(\theta_1 + 1)\left(\theta_2 - \frac{1}{\theta_1 + 1}\right)^2 + \frac{1}{\theta_1 + 1} \right) &\text{(Factor out } -1) \\
     &\propto \exp\left( -(\theta_1 + 1)\left(\theta_2 - \frac{1}{\theta_1 + 1}\right)^2\right)  &\text{(Remove constant)} \\
     &\propto \exp\left( -\frac{\left(\theta_2 - \frac{1}{\theta_1 + 1}\right)^2}{(\theta_1 + 1)^{-1}}\right) &\text{(Rearrange)}
\end{align}
$$

We're really close now. We have $\mu = \frac{1}{\theta_1 + 1}$.

Last thing, we gotta sort out that $1/2\sigma^2$ that we need.

$$
\begin{align}
\frac{1}{2\sigma^2} &= \theta_1+1 \\
\frac{1}{2} &= \sigma^2(\theta_1+1) \\
\sigma^2 &= \frac{1}{2(\theta_1+1)}
\end{align}
$$

Okay, so now we have the kernel of a normal distribution with $\mu = \frac{1}{\theta_1 + 1}, \sigma^2 = \frac{1}{2(\theta_1+1)}$. 

Therefore, 
$$\theta_2 \mid \theta_1, y \sim N\left(\mu = \frac{1}{\theta_1 + 1}, \sigma^2 = \frac{1}{2(\theta_1+1)}\right)$$

## (c)

Run a Gibbs sampler for $\theta_1$ and $\theta_2$ for 100,000 cycles.  Plot the estimated density of $p(\theta_1\mid y)$ and $p(\theta_2 \mid y)$ using these samples.

**Solution**

Using our answers from part a and b where:

$$
\begin{align}
    \theta_1 | \theta_2,y &\sim Gamma\left(3, \frac{1}{\theta_2^2 + 4}\right) \\
    \theta_2 | \theta_1,y &\sim N\left( \frac{1}{\theta_1+1}, \frac{1}{2(\theta_1+1)} \right)
\end{align}
$$

```{r}
#| code-fold: true
#| code-summary: Gibbs Sampler Code
gibbs <- function(B, theta) {
    # Function takes in 
    # - B: which is an integer to specify total iterations
    # - theta: a 2 length vector of parameter values
    
    # We want a check for theta_1 here. An indicator function isn't sufficient
    # as we aren't calculating density here. We need to throw an error and exit.
    if (theta[1] < 0) {
        stop(
            paste("Invalid parameter, theta_1 must be positive. Value: ", theta[1])
        )
    }
    
    # Create matrix for samples, ncol=2 due to 2 parameters
    theta_sims <- matrix(0, nrow=B+1, ncol=2)
    # Set first row to be starting values of theta
    theta_sims[1,] <- theta
    
    for (i in 2:(B+1)) {
        # Simulate from full conditional distribution for theta1
        # Note that our parameterization of the gamma uses the scale parameter, not rate
        alpha <- 3
        beta <- 1/(theta[2]^2 + 4)
        theta[1] <- rgamma(1, shape=alpha, scale=beta)
        
        # Determine full conditional mean for theta2
        mu_2 <- 1 / (theta[1] + 1)
        sigma_2 <- sqrt(1 / ( 2* (theta[1] + 1) ))
        theta[2] <- rnorm(1, mu_2, sigma_2)
        
        # Save sample
        theta_sims[i, ] <- theta
    }
    
    return(theta_sims)
}
```

Let's run a sanity check here.

```{r}
#| code-fold: true
#| code-summary: Trial run
#| 
theta <- c(0,0)
B <- 1000
chain1 <- gibbs(B, theta)

plot(chain1, pch = ".",
     xlab = expression(theta[1]),
     ylab = expression(theta[2]))
title("Samples from Gibbs sampler")
```


```{r}
#| code-fold: true
#| code-summary: Full Run

theta <- c(0,0)
B <- 100000
full_chain <- gibbs(B, theta)
gibbs_df <- as.data.frame(full_chain)
ggplot(gibbs_df) +
    geom_density(aes(x=V1, color="Theta 1"), linewidth=line_size) +
    geom_density(aes(x=V2, color="Theta 2"), linewidth=line_size) +
    theme_minimal() +
    labs(
        x = TeX(r'($\theta$)'), 
        y = "Density", 
        title = "Gibbs Sampling: Densities of Theta",
        color = ""
    ) +
    scale_color_discrete(labels = c(
        TeX(r'($p(\theta_1 | y)$)'), TeX(r'($p(theta_2 | y)$)')
    )) +
    scale_x_continuous(breaks=seq(-2,4,1))
```


## (d)

Determine the posterior mean, median, variance, and 95% central posterior interval for each parameter.

**Solution**

```{r}
#| code-fold: true
#| code-summary: Summary results of theta
quants <- c(0.025, 0.975)
gibbs_summary <- function(theta_samples, name) {
    theta_quants <- quantile(theta_samples, quants) |> round(5)
    cat(
        "--- Theta ", name, " Summary---\n",
        "Mean: ", round(mean(theta_samples), 5), "\n",
        "Median: ", round(median(theta_samples), 5), "\n",
        "Variance: ", round(var(theta_samples), 5), "\n",
        "95% Central Interval: [", theta_quants[1], ", ", theta_quants[2], "]\n\n",
        sep=""
    )
}
for (i in 1:2) {gibbs_summary(gibbs_df[,i], i)}

```


## (e)

Construct a plot showing the movement of the sample values for the first 100 cycles of the Gibbs sampler.  Label the axes appropriately.

**Solution**

```{r}
#| code-fold: true
#| code-summary: MCMC Path Plot
plot_mcmc_path(
  full_chain,
  ncycles = 100,
  xlim = c(-0.25, 2.5),
  ylim = c(-1.5, 2),
  xlab = expression(theta[1]),
  ylab = expression(theta[2]),
  main = "First 100 cycles of the chain"
)
```

For fun let's look at the last 100 cycles.

```{r}
#| code-fold: true
#| code-summary: MCMC Path Plot (Last 100 cycles)
plot_mcmc_path(
  full_chain[99000:100001, ],
  ncycles = 100,
  xlim = c(-0.25, 2.5),
  ylim = c(-1.5, 2),
  xlab = expression(theta[1]),
  ylab = expression(theta[2]),
  main = "Last 100 cycles of the chain"
)
```

# Problem 3

Consider the data in `coal.txt`, which contains counts for the number of coal mine disasters over a 112-year period (1851 to 1962) in the United Kingdom. The data have relatively high disaster counts in the early era, and relatively low counts in the later era. When did technology improvements and safety practices have an actual effect on the rate of serious accidents?

Assume that $y_1,y_2,\ldots,y_K\mid \lambda, K \stackrel{i.i.d.}{\sim} \mathrm{Poisson}(\lambda)$ and $y_{K+1},y_{K+2},\ldots,y_{112}\mid \phi, k \stackrel{i.i.d.}{\sim} \mathrm{Poisson}(\phi)$. 
In this model, $K$ is the year that the “change” occurred when technology and/or safety improvements changed the pattern of coal mine disasters. Our interest lies in finding the posterior distributions for $\lambda$, $\phi$, and $K$. Assume a Gamma(4, 1) prior distribution for $\lambda$, a Gamma(1, 2) prior distribution for $\phi$, and a discrete uniform prior distribution on $[1,2, \ldots,112]$ for $K$, i.e., $p_K(k)=1/112$ for $k\in\{1,2,\ldots,112\}$.  

## Dataset

First let's take a quick look at the data.

```{r}
coal <- read.table("../datasets/coal.txt", header = T)
str(coal)
summary(coal)
plot(x=coal$year, y=coal$disasters, type="l")
```

Okay so the number of disasters ranges from 0-6, our data ranges 112 years from 1851-1962 and things seem to drop off somewhat around 1900 or so. Cool.

## Setup

Let's go ahead and collect all of our known information. Priors have been simplified already to save time.

$$
\begin{align}
    y_1,y_2,\ldots,y_K\mid \lambda, K &\stackrel{i.i.d.}{\sim} \mathrm{Poisson}(\lambda) \\
    y_{K+1},y_{K+2},\ldots,y_{112}\mid \phi, k &\stackrel{i.i.d.}{\sim} \mathrm{Poisson}(\phi) \\
    \lambda &\sim \mathrm{Gamma}(4,1) \\
    \phi &\sim \mathrm{Gamma}(1,2) \\
    p(\lambda) &= \Gamma(4)^{-1} \lambda^3e^{-\lambda} \\
    p(\phi) &= \frac{1}{2} \exp\left(-\frac{\phi}{2}\right) \\
    p(k) &= \frac{1}{112} I_{k \in \{1,2,...,112\}}(k) \\
    I(\lambda) &= I_{\lambda \geq 0}(\lambda) \\
    I(\phi) &= I_{\phi \geq 0}(\phi)
\end{align}
$$

Next we need the "joint data density" for lack of a better descriptor. 

$$
\begin{align}
    p(\vec{y} \mid \lambda, k) &= \prod_{i=1}\frac{e^{-\lambda}\lambda^{y_i}}{y_i!}\\
    &= \frac{\exp(-k\lambda)\lambda^{\sum^k_1 y_i}}{\prod_{i=1}^ky_i}\\
    p(\vec{y} \mid \phi, k) &= \prod_{i=k+1}^{112}\frac{e^{-\phi}\phi^{y_i}}{y_i!}\\
    &= \frac{\exp(-(112-k)\phi)\phi^{\sum^{112}_{k+1} y_i}}{\prod_{i=k+1}^{112}y_i!}\\
    p(\vec{y} \mid \lambda, \phi, k) &= p(\vec{y} \mid \lambda, k) \cdot p(\vec{y} \mid  \phi, k) \\
    &= \frac{\exp(-k\lambda)\lambda^{\sum^k_1 y_i}}{\prod_{i=1}^ky_i!} \cdot \frac{\exp(-(112-k)\phi)\phi^{\sum^{112}_{k+1} y_i}}{\prod_{i=k+1}^{112}y_i!} \\
    &= \exp(-k\lambda - (112-k)\phi) \cdot \lambda^{\sum^k_1 y_i}\phi^{\sum^{112}_{k+1} y_i} \cdot \left(\prod_{i=1}^ky_i! \prod_{i=k+1}^{112}y_i!  \right)^{-1}
\end{align}
$$

Also for future reference to preserve my sanity,

$$
\begin{align}
    \sum^k_1 y_i &= S_1 \\
    \sum^{112}_{k+1} y_i &= S_2
\end{align}
$$

For sum1 and sum2 respectively.

## (a)

Determine the full conditional distribution for $\lambda$. It is a common distribution. Hint: The distribution will depend on a sum of the counts that depends on the value of $K$.

**Solution**

I skip some setup here to save time as it's getting late, but we include only the terms here that involve $\lambda$. 

$$
\begin{align}
    q(\lambda \mid \phi, k, \vec{y}) &\propto p(\vec{y} \mid \lambda, \phi, k) p(\lambda)\\
    &\propto \exp(-k\lambda) \lambda^{S_1} \lambda^3 e^{-\lambda}I(\lambda) \\
    &= \exp(-k\lambda - \lambda) \lambda^{3 + S_1}I(\lambda) \\
    &=  \lambda^{S_1 + 4 - 1} \exp(-\lambda(k+1))I(\lambda)
\end{align}
$$

Is the kernel of a gamma with $\alpha = S_1+4$ and $\beta = (k+1)^{-1}$. Therefore,

$$
    \lambda \mid \phi, k, \vec{y} \sim \mathrm{Gamma}(S_1+4, (k+1)^{-1})
$$

## (b)

Determine the full conditional distribution for $\phi$.  It is a common distribution. Hint: The distribution will depend on a sum of the counts that depends on the value of $K$.

**Solution**

Same stuff here, focus on what involves $\phi$

$$
\begin{align}
    q(\phi \mid \lambda, k, \vec{y}) &\propto p(\vec{y} \mid \lambda, \phi, k) p(\phi)\\
    &\propto \exp(-(112-k)\phi)\phi^{S_2} \exp\left(-\phi/2\right)I(\phi) \\
    &= \exp(-((112-k)\phi + 0.5\phi))\phi^{S_2}I(\phi) \\
    &= \exp(-\phi(112.5-k))\phi^{S^2}I(\phi)
\end{align}
$$

Kernel of a gamma with $\alpha=S_2 + 1$, $\beta = (112.5-k)^{-1}$. Therefore,

$$
    \phi \mid \lambda, k, \vec{y} \sim \mathrm{Gamma}(S_2 + 1, (112.5-k)^{-1})
$$

## (c)

Show that the full conditional distribution for $K$ is
$$p(K\mid\lambda,\phi,y)=\frac{\exp\left(K(\phi-\lambda)\right)\left(\frac{\lambda}{\phi}\right)^{\sum_{i=1}^K y_i}}{\sum_{K=1}^{112} \exp\left(K(\phi-\lambda)\right)\left(\frac{\lambda}{\phi}\right)^{\sum_{i=1}^K y_i}}.$$

**Solution**

Alright this is a long one. Again we focus on the terms that involve $k$. Note that the product of the factorials from the data distribution isn't here. That's because though it SEEMS to depend on $k$ the two of them actually are clean partitions of 1 to 112. So the combined product no longer depends on $k$. Therefore we don't include it after the proportional statement. I'd like to include more of the steps but I also don't want to do that! 

$$
\begin{align}
    q(k \mid \lambda, \phi, \vec{y}) &\propto p(\vec{y} \mid \lambda, \phi, k) p(k)\\
    &\propto \exp(-(k\lambda+(112-k)\phi)) \lambda^{S_1}\phi^{S_2} \\
    &= \exp(-(k\lambda + 112\phi - k\phi))\lambda^{S_1}\phi^{S_2} \\
    &= \exp(k(\phi-\lambda) - 112\phi) \lambda^{S_1}\phi^{S_2} \\ 
    &= \exp(k(\phi-\lambda)) \cdot \exp(-112\phi) \lambda^{S_1}\phi^{S_2} \\
    &\propto \exp(k(\phi-\lambda))  \lambda^{S_1}\phi^{S_2} \\
\end{align}
$$

Okay so we're close to the form of the numerator but not quite. Let's take note of the summations $S_1$ and $S_2$. These neatly partition the full sum. So we can do some clever algebraic rearranging to get the form we need. We want $S_1$ to be in the exponent in the quotient involving $\lambda/\phi$. So let's go towards that.

$$
\begin{align}
\sum_{i=1}^{112} y_i &= \sum_{i=1}^{k} y_i + \sum_{i=k+1}^{112} y_i \\
\sum_{i=k+1}^{112} y_i &= \sum_{i=1}^{112} y_i - \sum_{i=1}^{k} y_i \\
\end{align}
$$

Using this knowledge, we return,

$$
\begin{align}
    q(k \mid \lambda, \phi, \vec{y}) &\propto \exp(k(\phi-\lambda))  \lambda^{S_1}\phi^{S_2} \\
    &= \exp(k(\phi-\lambda)) \lambda^{S_1}\phi^{\sum_{i=1}^{112} - S_1} \\
    &= \exp(k(\phi-\lambda)) \lambda^{S_1}\phi^{\sum_{i=1}^{112}}\phi^{- S_1} \\
    &= \exp(k(\phi-\lambda)) \left(\frac{\lambda}{\phi}\right)^{S_1} \phi^{\sum_{i=1}^{112}} \\
\end{align}
$$

Alright so I got stuck here for a while not knowing what to do next. The hint here is to really look at the denominator and think about what it is. 

Our numerator gives us the density for a specific k. To get our probability we need to compare that to the density of every possible k. That's what the denominator provides, it's there due to the total law of probability. It is our normalizing constant! So, we take that unnormalized distribution and sum over all possible values of k. That's all it is. Note we swap now from $q(k)$ to $p(k)$.

$$
\begin{align}
    p(k \mid \lambda, \phi, \vec{y}) &= \frac{\exp(k(\phi-\lambda)) \left(\frac
    {\lambda}{\phi}\right)^{S_1} \phi^{\sum_{i=1}^{112}}}
    {\sum_{k=1}^{112} \exp(k(\phi-\lambda)) \left(\frac{\lambda}{\phi}\right)^{S_1} \phi^{\sum_{i=1}^{112}}} \\
    &= \frac{\exp(k(\phi-\lambda)) \left(\frac
    {\lambda}{\phi}\right)^{S_1}}
    {\sum_{k=1}^{112} \exp(k(\phi-\lambda)) \left(\frac{\lambda}{\phi}\right)^{S_1}} \\
    &= \frac{\exp(k(\phi-\lambda)) \left(\frac
    {\lambda}{\phi}\right)^{\sum_{i=1}^k}}
    {\sum_{k=1}^{112} \exp(k(\phi-\lambda)) \left(\frac{\lambda}{\phi}\right)^{\sum_{i=1}^k}} \\
\end{align}
$$

And with that we have the same form as our goal!

## (d)

Run a 100,000 cycle Gibbs sampler using the distributions determined in parts (a) through (c) to approximate the posterior distribution of the three parameters.  Plot the estimated density/mass function of each parameter.

**Solution**

```{r}
#| code-fold: true
#| code-summary: First let's get those sums pre-computed. 
#| 
n <- nrow(coal)
sums <- matrix(nrow=n, ncol=2)

for (k in 1:n) {
    sums[k,1] <- sum(coal[1:k, 2])
    sums[k,2] <- sum(coal[(k+1):n, 2])
}

sums <- as.data.frame(sums)
colnames(sums) <- c("sum1", "sum2")
```

Next we need to be able to sample from the pmf of K given our ever changing values of $\lambda$ and $\phi$. We want a function for that.

```{r}
#| code-fold: true
#| code-summary: Generate function to sample from the full conditional of K
#| 
sample_k_full_cond <- function(lambda, phi, n=112) {
    # The main goal here is that we need to assign probabilities to each k
    #   so that we actually have a distribution to sample from. We don't have
    #   a convenient rnorm or rbeta so we need to make our own.
    
    numerators <- vector(length=n)
    denominator <- 0
    
    for (k in 1:n) {
        
        # Split numerator into two parts for readability
        part_1 <- exp(k * (phi - lambda))
        # (lambda/phi)^(sum from i:k y_i) rewritten: (we use e^(ln(x)) to sidestep some gnarly exponents)
        part_2 <- exp(sums$sum1[k]*log(lambda / phi))
        
        denominator <- denominator + (part_1 * part_2)
        numerators[k] <- part_1 * part_2
    }
    
    pmf <- numerators / denominator
    
    # Sample from this new discrete distribution!
    sample(1:n, size=1, prob = pmf)
}
```

```{r}
#| code-fold: true
#| code-summary: Gibbs Sampler Code (Reworked)
gibbs <- function(B, theta) {
    # Function takes in 
    # - B: which is an integer to specify total iterations
    # - theta: a 3 length vector of parameter values
    #   - (lambda, phi, K) in that order
    
    # Create matrix for samples, ncol=3 due to 3 parameters
    theta_sims <- matrix(0, nrow=B+1, ncol=3)
    # Set first row to be starting values of theta
    theta_sims[1,] <- theta
    n <- nrow(coal)
    
    for (i in 2:(B+1)) {
        
        # Simulate from full conditional distribution for Lambda (theta_1)
        # Note that our parameterization of the gamma uses the scale parameter, not rate
        alpha <- sums$sum1[theta[3]] + 4
        beta <- 1/(theta[3]+1)
        theta[1] <- rgamma(1, shape=alpha, scale=beta)
        
        # Full coniditional for Phi (theta_2)
        alpha <- sums$sum2[theta[3]] + 1
        beta <- 1/(n + 0.5 - theta[3])
        theta[2] <- rgamma(1, shape=alpha, scale=beta)
        
        theta[3] <- sample_k_full_cond(lambda=theta[1], phi=theta[2])
        
        # Save simulations
        theta_sims[i, ] <- theta
    }
    
    return(theta_sims)
}
```

```{r}
#| code-fold: true
#| code-summary: Trial run and basic line plot
#| 
theta <- c(0,0, 1)
B <- 100
chain1 <- gibbs(B, theta)

plot(
    chain1[,3], 
    type="l",
    ylab="Year Index",
    xlab="Chain Index",
    main="Value of K per Iteration"
)
```

Okay nothing crazy going on and we seem to be somewhat converging. Cool, let's do the full run.

```{r, warning=FALSE}
#| code-fold: true
#| code-summary: Full run and plots
#| 
theta <- c(0,0, 1)
B <- 100000
full_chain <- gibbs(B, theta)
gibbs_df <- as.data.frame(full_chain)

ggplot(gibbs_df) +
    geom_density(aes(x=V1), linewidth=line_size) +
    theme_minimal() +
    labs(
        x = TeX(r'($\lambda$)'),
        y = "Density",
        title = "Gibbs Sampling: Density of Lambda"
    )

ggplot(gibbs_df) +
    geom_density(aes(x=V2), linewidth=line_size) +
    theme_minimal() +
    labs(
        x = TeX(r'($\phi$)'),
        y = "Density",
        title = "Gibbs Sampling: Density of Phi"
    )

ggplot(gibbs_df, aes(x=V3)) +
    geom_bar(aes(y=after_stat(count) / sum(after_stat(count))) ) +
    theme_minimal() +
    labs(
        x = TeX(r'($K$)'), 
        y = "Relative Frequency", 
        title = "Gibbs Sampling: Density of K"
    ) +
    xlim(c(25, 55))
```

## (e)

Determine the posterior mean, median, variance, and 95% central posterior interval for each parameter.

**Solution**

Thank goodness we wrote a function for this.

Reminder, $\theta = (\lambda, \phi, k)$. So $\theta_1 = \lambda$ and so on.

```{r}
for (i in 1:3) {gibbs_summary(gibbs_df[,i], i)}
```

## (f)

Let $m$ be the median of the posterior distribution for $K$.  Create a time series plot of the counts where the first $m$ years are shown in blue, while the remaining years are shown in orange.  This distinguishes the years where we have a $\mathrm{Poisson}(\lambda)$ distribution from a $\mathrm{Poisson}(\phi)$ distribution.  

**Solution**

```{r}
#| code-fold: true
#| code-summary: More Plotting!

m <- median(gibbs_df[, 3])
coal <- coal %>% 
    mutate(period = ifelse(year <= min(coal$year) + m, "before_k", "after_k"))

ggplot(coal, aes(x=year, y=disasters, color=period)) +
    geom_line() +
    geom_point(size = 2) +
    labs(x="", y="Disasters", title="Number of Disasters - Before and After K") +
    theme_minimal() +
    theme(legend.position = "none")
```

