---
title: "Testing Modeling"
author: "Brady Lamson"
format: html
---

Quick reference for variable selection tips
[link](https://jwmi.github.io/BMB/16-Variable-selection.pdf)

```{r}
#| Echo: False
#| Warning: False
#| 
source("../scripts/helpers.R")
packages <- c(
    "ggplot2", "dplyr", "readr", "fastDummies", "janitor", "tidyr",
    "rstanarm",  "qgraph", "corrr", "loo", "bayesplot", "tidybayes"
)
load_and_install_packages(packages)
```

# Data Ingestion

We do data cleaning AFTER reading in a file because a lot of this info can't be saved into a csv without inflating file size.

```{r}
# Seed reresents the 100% speedrun of Baten Kaitos which took 338 hours, 43 minutes and 26 seconds
set.seed(3384326)

# Used for stratified sampling 
cat_columns <- c("hispanic", "spm_poor", "sex", "education", "race", "region", "married")

# Used for dummy variable creation
factor_columns <- setdiff(cat_columns, c("hispanic", "spm_poor", "married"))

df <- readr::read_csv("../data/poverty_data.csv.gz") %>%
    # --- SOME DATA CLEANING HERE ---
    # turn mar into 0/1 married or not married to make use of unbalanced categories
    mutate(married = (mar == 1) + 0) %>%
    add_state_abbreviations() %>%
    add_region() %>%
    apply_factor_labels() %>%
    select(-c(state, state_code, wt, mar)) %>%
    # --- SCALE NUMERIC COLUMNS ---
    mutate(
        hi_premium = scale(hi_premium),
        moop_other = scale(moop_other),
        agi = scale(agi),
        spm_povthreshold = scale(spm_povthreshold)
    ) %>%
    # --- STRATIFICATION AND DUMMY VARIABLE CREATION ---
    stratified_sample(cat_columns = cat_columns, prop=0.005) %>%
    # Save dummies for after stratification as they greatly increase dimensionality
    fastDummies::dummy_cols(
        select_columns = factor_columns,
        remove_selected_columns = TRUE,
        remove_first_dummy = TRUE,
        ignore_na = TRUE
    ) %>%
    clean_names()
```

## Get factor columns
```{r}
columns <- df %>% colnames()

education_columns <- columns %>%
    regmatches(., gregexpr("^education_.*", .)) %>% unlist()

region_columns <- columns %>%
    regmatches(., gregexpr("^region_.*", .)) %>% unlist()

race_columns <- columns %>%
    regmatches(., gregexpr("^race_.*", .)) %>% unlist()
```

# Modeling

```{r, include=FALSE}
glmod_logit_full <- stan_glm(
    reformulate(
        c(
            "age", "married", "agi",
            "spm_povthreshold", "hispanic", "hi_premium",
            "moop_other", "sex_female",
            education_columns, region_columns, race_columns
        ), 
        response = "spm_poor"
    ), 
    prior=lasso(),
    data=df, iter=10000, chains=8, seed=100, cores=8
)
```

```{r}
modsum <- summary(glmod_logit_full)
modsum
```

```{r}
posterior_interval(glmod_logit_full) %>% exp()
```

```{r}
coef(glmod_logit_full) %>% exp()
```

```{r}
posterior_samples <- as.matrix(glmod_logit_full)

mcmc_intervals(
    exp(posterior_samples), 
    pars = names(coef(glmod_logit_full))
) +
    ggtitle("Posterior intervals for odds ratios") +
    geom_vline(xintercept = 1, linetype = "dashed")

rm(posterior_samples)
```


## Model 2 

LETS REMOVE SOME OF THE BAD ONES and rerun the lasso

```{r, include=FALSE}
glmod_logit_2 <- update(
  glmod_logit_full,
  formula = reformulate(
    c(
      "married", "agi",
      "spm_povthreshold", "hispanic",
      "moop_other", "sex_female",
      education_columns, region_columns, race_columns
    ),
    response = "spm_poor"
  )#,
  #prior = NULL  # Optional: removes the lasso prior
)
```

```{r}
posterior_interval(glmod_logit_2) %>% exp()
```

```{r}
posterior_samples <- as.matrix(glmod_logit_2)

mcmc_intervals(
    exp(posterior_samples), 
    pars = names(coef(glmod_logit_2))
) +
    ggtitle("Posterior intervals for odds ratios") +
    geom_vline(xintercept = 1, linetype = "dashed")

rm(posterior_samples)
```

# Model 3

```{r, include=FALSE}
glmod_logit_3 <- update(
  glmod_logit_full,
  formula = reformulate(
    c(
      "married", "agi",
      "spm_povthreshold",
      education_columns, region_columns, race_columns
    ),
    response = "spm_poor"
  ),
  prior = NULL  # Optional: removes the lasso prior
)
```

```{r}
posterior_samples <- as.matrix(glmod_logit_3)

mcmc_intervals(
    exp(posterior_samples)[,2:14], 
    pars = setdiff(names(coef(glmod_logit_3)), "(Intercept)")
) +
    ggtitle("Posterior intervals for odds ratios") +
    geom_vline(xintercept = 1, linetype = "dashed")

rm(posterior_samples)
```

# Correlation stuff

To justify removing hispanic from the model, we point out that it's highly correlated with race_other and the latter likely contains a lot of the same information.

```{r}
qgraph(cor(df), minimum=0.25)
```

```{r}
library(corrr)
library(dplyr)

# Select only numeric columns (since you're using a tibble)
df_numeric <- df %>% select(where(is.numeric))

# Create a correlation matrix and reorder
cor_mat <- correlate(df_numeric)

# Print nicely
print(cor_mat)

# For visual inspection
rplot(cor_mat)  # heatmap-style

```

# WAIC Scores

```{r}
ll_full <- log_lik(glmod_logit_full)
print(waic(ll_full, cores=8))
rm(ll_full)
```

```{r}
ll_2 <- log_lik(glmod_logit_2)
waic(ll_2, cores=8)
rm(ll_2)
```

```{r}
ll_3 <- log_lik(glmod_logit_3)
waic(ll_3, cores=8)
rm(ll_3)
```

## Comparing them

| Model         | WAIC   | SE    | p\_waic | elpd\_waic |
| ------------- | ------ | ----- | ------- | ---------- |
| Full          | 8154.6 | 256.3 | 21.5    | -4077.3    |
| Model 2       | 8168.6 | 256.3 | 19.8    | -4084.3    |
| Model 3       | 8209.7 | 256.2 | 16.2    | -4104.8    |

| Models | Diff|
|---|---|
|Full - 3| -55.1|
|Full - 2| -14. |
|2 - 3   | -41.1|

The loss in WAIC doesn't seem substantial going from the full model to model 3 because the difference falls well within the standard error of the WAICs. Thus there isn't substantial evidence to indicate that the full model is an improvement over the simplified model. 

# Final Model

So our final model has the covariates: married, agi, pov_threshold, region, race, education.
