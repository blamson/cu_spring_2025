---
title: "Testing Modeling"
author: "Brady Lamson"
format: html
---

Quick reference for variable selection tips
[link](https://jwmi.github.io/BMB/16-Variable-selection.pdf)

```{r}
#| Echo: False
#| Warning: False
#| 
source("../scripts/helpers.R")
packages <- c(
    "ggplot2", "dplyr", "readr", "fastDummies", "janitor", "tidyr",
    "rstanarm",  "qgraph", "corrr", "loo", "bayesplot", "tidybayes",
    "latex2exp"
)
load_and_install_packages(packages)
```

# Data Ingestion

We do data cleaning AFTER reading in a file because a lot of this info can't be saved into a csv without inflating file size.

```{r}
# Seed reresents the 100% speedrun of Baten Kaitos which took 338 hours, 43 minutes and 26 seconds
set.seed(3384326)

# Used for stratified sampling 
cat_columns <- c("hispanic", "spm_poor", "sex", "education", "race", "region", "married")

# Used for dummy variable creation
factor_columns <- setdiff(cat_columns, c("hispanic", "spm_poor", "married"))

df <- readr::read_csv("../data/poverty_data.csv.gz") %>%
    # --- SOME DATA CLEANING HERE ---
    # turn mar into 0/1 married or not married to make use of unbalanced categories
    mutate(married = (mar == 1) + 0) %>%
    add_state_abbreviations() %>%
    add_region() %>%
    # Remove NA education rows
    filter(education != 0) %>%
    apply_factor_labels() %>%
    select(-c(state, state_code, wt, mar)) %>%
    # --- SCALE NUMERIC COLUMNS ---
    # mutate(
    #     hi_premium = scale(hi_premium),
    #     moop_other = scale(moop_other),
    #     agi = scale(agi),
    #     spm_povthreshold = scale(spm_povthreshold)
    # ) %>%
    # --- STRATIFICATION AND DUMMY VARIABLE CREATION ---
    stratified_sample(cat_columns = cat_columns, prop=0.005) %>%
    # Save dummies for after stratification as they greatly increase dimensionality
    fastDummies::dummy_cols(
        select_columns = factor_columns,
        remove_selected_columns = TRUE,
        remove_first_dummy = TRUE,
        ignore_na = TRUE
    ) %>%
    clean_names()
```

## Get factor columns
```{r}
columns <- df %>% colnames()

education_columns <- columns %>%
    regmatches(., gregexpr("^education_.*", .)) %>% unlist()

region_columns <- columns %>%
    regmatches(., gregexpr("^region_.*", .)) %>% unlist()

race_columns <- columns %>%
    regmatches(., gregexpr("^race_.*", .)) %>% unlist()
```

# Modeling

```{r, include=FALSE}
glmod_logit_full <- stan_glm(
    reformulate(
        c(
            "age", "married", "agi",
            "spm_povthreshold", "hispanic", "hi_premium",
            "moop_other", "sex_female",
            education_columns, region_columns, race_columns
        ), 
        response = "spm_poor"
    ), 
    family = binomial(link = "logit"),
    data=df, iter=5000, chains=4, seed=100, cores=4
)
```

```{r}
prior_summary(glmod_logit_full)
```


```{r}
modsum <- summary(glmod_logit_full)
modsum
```

```{r}
posterior_interval(glmod_logit_full) %>% exp()
```

```{r}
coef(glmod_logit_full) %>% exp()
```

```{r}
posterior_samples <- as.matrix(glmod_logit_full)

mcmc_intervals(
    exp(posterior_samples), 
    pars = names(coef(glmod_logit_full))
) +
    ggtitle("Posterior intervals for odds ratios") +
    geom_vline(xintercept = 1, linetype = "dashed")

rm(posterior_samples)
```


## Model 2 

LETS REMOVE SOME OF THE BAD ONES and rerun the lasso

```{r, include=FALSE}
glmod_logit_2 <- update(
  glmod_logit_full,
  formula = reformulate(
    c(
      "married", "hispanic", "sex_female",
      education_columns, region_columns, race_columns
    ),
    response = "spm_poor"
  )
)
```

```{r}
posterior_interval(glmod_logit_2) %>% exp()
```

```{r}
posterior_samples <- as.matrix(glmod_logit_2)

mcmc_intervals(
    exp(posterior_samples), 
    pars = names(coef(glmod_logit_2))
) +
    ggtitle("Posterior intervals for odds ratios") +
    geom_vline(xintercept = 1, linetype = "dashed")

rm(posterior_samples)
```

# Model 3

```{r, include=FALSE}
glmod_logit_3 <- update(
  glmod_logit_full,
  formula = reformulate(
    c(
      "married", "sex_female",
      education_columns, region_columns, race_columns
    ),
    response = "spm_poor"
  )
)
```

```{r}
posterior_samples <- as.matrix(glmod_logit_3)

mcmc_intervals(
    exp(posterior_samples), 
    pars = setdiff(names(coef(glmod_logit_3)), "(Intercept)")
) +
    ggtitle("Posterior intervals for odds ratios") +
    geom_vline(xintercept = 1, linetype = "dashed")

rm(posterior_samples)
```

# Correlation stuff

To justify removing hispanic from the model, we point out that it's highly correlated with race_other and the latter likely contains a lot of the same information.

```{r}
qgraph(cor(df), minimum=0.25)
```

```{r}
library(corrr)
library(dplyr)

# Select only numeric columns (since you're using a tibble)
df_numeric <- df %>% select(where(is.numeric))

# Create a correlation matrix and reorder
cor_mat <- correlate(df_numeric)

# Print nicely
print(cor_mat)

# For visual inspection
rplot(cor_mat)  # heatmap-style

```

# WAIC Scores

```{r}
ll_full <- log_lik(glmod_logit_full)
print(waic(ll_full, cores=8))
rm(ll_full)
```

```{r}
ll_2 <- log_lik(glmod_logit_2)
waic(ll_2, cores=8)
rm(ll_2)
```

```{r}
ll_3 <- log_lik(glmod_logit_3)
waic(ll_3, cores=8)
rm(ll_3)
```

## Comparing them

| Model         | WAIC   | SE    | 
| ------------- | ------ | ----- | 
| Full          | 5094 | 117.2 | 
| Model 2       | 5094 | 117.2 | 
| Model 3       | 5107.4 | 117.1 |

| Models | Diff|
|---|---|
|Full - 3| -13.4|
|Full - 2| 0 |
|2 - 3   | -13.4|

The loss in WAIC doesn't seem substantial going from the full model to model 3 because the difference falls well within the standard error of the WAICs. Thus there isn't substantial evidence to indicate that the full model is an improvement over the simplified model. Funnily enough the full model and second model are basically identical here. 

$R_b^2$

```{r}
model1_rb2 <- bayes_R2(glmod_logit_full)
model2_rb2 <- bayes_R2(glmod_logit_2)
model3_rb2 <- bayes_R2(glmod_logit_3)
```

```{r}
ggplot() +
    geom_density(aes(x=model1_rb2, color = "model 1"), linewidth=1.1) +
    geom_density(aes(x=model2_rb2, color = "model 2"), linewidth=1.1) +
    geom_density(aes(x=model3_rb2, color = "model 3"), linewidth=1.1) +
    labs(
        x = TeX(r'($R_b^2$)'),
        color = "",
        title = "Comparing Model Performance"
    ) +
    theme_minimal()
```

